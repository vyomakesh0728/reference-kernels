[SCALE DEBUG] sfa_ref_cpu shape=torch.Size([7168, 1024, 1]), device=cuda:0
[SCALE DEBUG] sfb_ref_cpu shape=torch.Size([128, 1024, 1]), device=cuda:0
[DEBUG] No padding needed, K_scales=1024
[DEBUG] a_bytes: shape=(1, 7168, 8192), stride=(58720256, 8192, 1), elem_size=1, numel=58720256, bytes=58720256, data_ptr=0x7ffc2a000000
[DEBUG] b_bytes: shape=(1, 128, 8192), stride=(1048576, 8192, 1), elem_size=1, numel=1048576, bytes=1048576, data_ptr=0x7ffc65500000
[DEBUG] sfa_bytes: shape=(1, 7168, 1024), stride=(7340032, 1024, 1), elem_size=1, numel=7340032, bytes=7340032, data_ptr=0x7ffc28000000
[DEBUG] sfb_bytes: shape=(1, 128, 1024), stride=(131072, 1024, 1), elem_size=1, numel=131072, bytes=131072, data_ptr=0x7ffc5fb83800
[DEBUG] sfa_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 0, 64, 64, 0, 64, 64, 56]
[DEBUG] sfa_bytes[0,1,:8] (batch 0, row 1, first 8 scales): [0, 56, 56, 0, 64, 56, 56, 64]
[DEBUG] sfa_bytes[0,2,:8] (batch 0, row 2, first 8 scales): [0, 64, 64, 56, 0, 64, 56, 0]
[DEBUG] sfa_bytes[0,3,:8] (batch 0, row 3, first 8 scales): [64, 64, 0, 56, 56, 56, 56, 56]
[DEBUG] sfa_bytes[0,4,:8] (batch 0, row 4, first 8 scales): [56, 56, 64, 64, 0, 0, 0, 64]
[DEBUG] sfb_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 56, 56, 0, 64, 64, 64, 56]
[DEBUG] a_bytes range: [0x7ffc2a000000, 0x7ffc2d800000) (58720256 bytes)
[DEBUG] b_bytes range: [0x7ffc65500000, 0x7ffc65600000) (1048576 bytes)
[DEBUG] sfa_bytes range: [0x7ffc28000000, 0x7ffc28700000) (7340032 bytes)
[DEBUG] sfb_bytes range: [0x7ffc5fb83800, 0x7ffc5fba3800) (131072 bytes)
[DEBUG] a shape=(1, 7168, 8192), stride=(58720256, 8192, 1), data_ptr=0x7ffc2a000000
[DEBUG] b shape=(1, 128, 8192), stride=(1048576, 8192, 1), data_ptr=0x7ffc65500000
âœ…(Python) a_bytes 128-byte alignment check passed: 0x7ffc2a000000
âœ…(Python) b_bytes 128-byte alignment check passed: 0x7ffc65500000
LAUNCH DEBUG: M=7168 K=16384 L=1
LAUNCH DEBUG: using fp4_gemv_rank2_cta

[SCALE DEBUG] sfa_ref_cpu shape=torch.Size([7168, 1024, 1]), device=cuda:0
[SCALE DEBUG] sfb_ref_cpu shape=torch.Size([128, 1024, 1]), device=cuda:0
[DEBUG] No padding needed, K_scales=1024
[DEBUG] a_bytes: shape=(1, 7168, 8192), stride=(58720256, 8192, 1), elem_size=1, numel=58720256, bytes=58720256, data_ptr=0x7ffc2a000000
[DEBUG] b_bytes: shape=(1, 128, 8192), stride=(1048576, 8192, 1), elem_size=1, numel=1048576, bytes=1048576, data_ptr=0x7ffc65500000
[DEBUG] sfa_bytes: shape=(1, 7168, 1024), stride=(7340032, 1024, 1), elem_size=1, numel=7340032, bytes=7340032, data_ptr=0x7ffc28000000
[DEBUG] sfb_bytes: shape=(1, 128, 1024), stride=(131072, 1024, 1), elem_size=1, numel=131072, bytes=131072, data_ptr=0x7ffc5fb83800
[DEBUG] sfa_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 0, 64, 64, 0, 64, 64, 56]
[DEBUG] sfa_bytes[0,1,:8] (batch 0, row 1, first 8 scales): [0, 56, 56, 0, 64, 56, 56, 64]
[DEBUG] sfa_bytes[0,2,:8] (batch 0, row 2, first 8 scales): [0, 64, 64, 56, 0, 64, 56, 0]
[DEBUG] sfa_bytes[0,3,:8] (batch 0, row 3, first 8 scales): [64, 64, 0, 56, 56, 56, 56, 56]
[DEBUG] sfa_bytes[0,4,:8] (batch 0, row 4, first 8 scales): [56, 56, 64, 64, 0, 0, 0, 64]
[DEBUG] sfb_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 56, 56, 0, 64, 64, 64, 56]
[DEBUG] a_bytes range: [0x7ffc2a000000, 0x7ffc2d800000) (58720256 bytes)
[DEBUG] b_bytes range: [0x7ffc65500000, 0x7ffc65600000) (1048576 bytes)
[DEBUG] sfa_bytes range: [0x7ffc28000000, 0x7ffc28700000) (7340032 bytes)
[DEBUG] sfb_bytes range: [0x7ffc5fb83800, 0x7ffc5fba3800) (131072 bytes)
[DEBUG] a shape=(1, 7168, 8192), stride=(58720256, 8192, 1), data_ptr=0x7ffc2a000000
[DEBUG] b shape=(1, 128, 8192), stride=(1048576, 8192, 1), data_ptr=0x7ffc65500000
âœ…(Python) a_bytes 128-byte alignment check passed: 0x7ffc2a000000
âœ…(Python) b_bytes 128-byte alignment check passed: 0x7ffc65500000
LAUNCH DEBUG: M=7168 K=16384 L=1
LAUNCH DEBUG: using fp4_gemv_rank2_cta

[SCALE DEBUG] sfa_ref_cpu shape=torch.Size([7168, 1024, 1]), device=cuda:0
[SCALE DEBUG] sfb_ref_cpu shape=torch.Size([128, 1024, 1]), device=cuda:0
[DEBUG] No padding needed, K_scales=1024
[DEBUG] a_bytes: shape=(1, 7168, 8192), stride=(58720256, 8192, 1), elem_size=1, numel=58720256, bytes=58720256, data_ptr=0x7ffc2a000000
[DEBUG] b_bytes: shape=(1, 128, 8192), stride=(1048576, 8192, 1), elem_size=1, numel=1048576, bytes=1048576, data_ptr=0x7ffc65500000
[DEBUG] sfa_bytes: shape=(1, 7168, 1024), stride=(7340032, 1024, 1), elem_size=1, numel=7340032, bytes=7340032, data_ptr=0x7ffc28000000
[DEBUG] sfb_bytes: shape=(1, 128, 1024), stride=(131072, 1024, 1), elem_size=1, numel=131072, bytes=131072, data_ptr=0x7ffc5fb83800
[DEBUG] sfa_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 0, 64, 64, 0, 64, 64, 56]
[DEBUG] sfa_bytes[0,1,:8] (batch 0, row 1, first 8 scales): [0, 56, 56, 0, 64, 56, 56, 64]
[DEBUG] sfa_bytes[0,2,:8] (batch 0, row 2, first 8 scales): [0, 64, 64, 56, 0, 64, 56, 0]
[DEBUG] sfa_bytes[0,3,:8] (batch 0, row 3, first 8 scales): [64, 64, 0, 56, 56, 56, 56, 56]
[DEBUG] sfa_bytes[0,4,:8] (batch 0, row 4, first 8 scales): [56, 56, 64, 64, 0, 0, 0, 64]
[DEBUG] sfb_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 56, 56, 0, 64, 64, 64, 56]
[DEBUG] a_bytes range: [0x7ffc2a000000, 0x7ffc2d800000) (58720256 bytes)
[DEBUG] b_bytes range: [0x7ffc65500000, 0x7ffc65600000) (1048576 bytes)
[DEBUG] sfa_bytes range: [0x7ffc28000000, 0x7ffc28700000) (7340032 bytes)
[DEBUG] sfb_bytes range: [0x7ffc5fb83800, 0x7ffc5fba3800) (131072 bytes)
[DEBUG] a shape=(1, 7168, 8192), stride=(58720256, 8192, 1), data_ptr=0x7ffc2a000000
[DEBUG] b shape=(1, 128, 8192), stride=(1048576, 8192, 1), data_ptr=0x7ffc65500000
âœ…(Python) a_bytes 128-byte alignment check passed: 0x7ffc2a000000
âœ…(Python) b_bytes 128-byte alignment check passed: 0x7ffc65500000
LAUNCH DEBUG: M=7168 K=16384 L=1
LAUNCH DEBUG: using fp4_gemv_rank2_cta

[SCALE DEBUG] sfa_ref_cpu shape=torch.Size([7168, 1024, 1]), device=cuda:0
[SCALE DEBUG] sfb_ref_cpu shape=torch.Size([128, 1024, 1]), device=cuda:0
[DEBUG] No padding needed, K_scales=1024
[DEBUG] a_bytes: shape=(1, 7168, 8192), stride=(58720256, 8192, 1), elem_size=1, numel=58720256, bytes=58720256, data_ptr=0x7ffc2a000000
[DEBUG] b_bytes: shape=(1, 128, 8192), stride=(1048576, 8192, 1), elem_size=1, numel=1048576, bytes=1048576, data_ptr=0x7ffc65500000
[DEBUG] sfa_bytes: shape=(1, 7168, 1024), stride=(7340032, 1024, 1), elem_size=1, numel=7340032, bytes=7340032, data_ptr=0x7ffc28000000
[DEBUG] sfb_bytes: shape=(1, 128, 1024), stride=(131072, 1024, 1), elem_size=1, numel=131072, bytes=131072, data_ptr=0x7ffc5fb83800
[DEBUG] sfa_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 0, 64, 64, 0, 64, 64, 56]
[DEBUG] sfa_bytes[0,1,:8] (batch 0, row 1, first 8 scales): [0, 56, 56, 0, 64, 56, 56, 64]
[DEBUG] sfa_bytes[0,2,:8] (batch 0, row 2, first 8 scales): [0, 64, 64, 56, 0, 64, 56, 0]
[DEBUG] sfa_bytes[0,3,:8] (batch 0, row 3, first 8 scales): [64, 64, 0, 56, 56, 56, 56, 56]
[DEBUG] sfa_bytes[0,4,:8] (batch 0, row 4, first 8 scales): [56, 56, 64, 64, 0, 0, 0, 64]
[DEBUG] sfb_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 56, 56, 0, 64, 64, 64, 56]
[DEBUG] a_bytes range: [0x7ffc2a000000, 0x7ffc2d800000) (58720256 bytes)
[DEBUG] b_bytes range: [0x7ffc65500000, 0x7ffc65600000) (1048576 bytes)
[DEBUG] sfa_bytes range: [0x7ffc28000000, 0x7ffc28700000) (7340032 bytes)
[DEBUG] sfb_bytes range: [0x7ffc5fb83800, 0x7ffc5fba3800) (131072 bytes)
[DEBUG] a shape=(1, 7168, 8192), stride=(58720256, 8192, 1), data_ptr=0x7ffc2a000000
[DEBUG] b shape=(1, 128, 8192), stride=(1048576, 8192, 1), data_ptr=0x7ffc65500000
âœ…(Python) a_bytes 128-byte alignment check passed: 0x7ffc2a000000
âœ…(Python) b_bytes 128-byte alignment check passed: 0x7ffc65500000
LAUNCH DEBUG: M=7168 K=16384 L=1
LAUNCH DEBUG: using fp4_gemv_rank2_cta

[SCALE DEBUG] sfa_ref_cpu shape=torch.Size([7168, 1024, 1]), device=cuda:0
[SCALE DEBUG] sfb_ref_cpu shape=torch.Size([128, 1024, 1]), device=cuda:0
[DEBUG] No padding needed, K_scales=1024
[DEBUG] a_bytes: shape=(1, 7168, 8192), stride=(58720256, 8192, 1), elem_size=1, numel=58720256, bytes=58720256, data_ptr=0x7ffc2a000000
[DEBUG] b_bytes: shape=(1, 128, 8192), stride=(1048576, 8192, 1), elem_size=1, numel=1048576, bytes=1048576, data_ptr=0x7ffc65500000
[DEBUG] sfa_bytes: shape=(1, 7168, 1024), stride=(7340032, 1024, 1), elem_size=1, numel=7340032, bytes=7340032, data_ptr=0x7ffc28000000
[DEBUG] sfb_bytes: shape=(1, 128, 1024), stride=(131072, 1024, 1), elem_size=1, numel=131072, bytes=131072, data_ptr=0x7ffc5fb83800
[DEBUG] sfa_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 0, 64, 64, 0, 64, 64, 56]
[DEBUG] sfa_bytes[0,1,:8] (batch 0, row 1, first 8 scales): [0, 56, 56, 0, 64, 56, 56, 64]
[DEBUG] sfa_bytes[0,2,:8] (batch 0, row 2, first 8 scales): [0, 64, 64, 56, 0, 64, 56, 0]
[DEBUG] sfa_bytes[0,3,:8] (batch 0, row 3, first 8 scales): [64, 64, 0, 56, 56, 56, 56, 56]
[DEBUG] sfa_bytes[0,4,:8] (batch 0, row 4, first 8 scales): [56, 56, 64, 64, 0, 0, 0, 64]
[DEBUG] sfb_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 56, 56, 0, 64, 64, 64, 56]
[DEBUG] a_bytes range: [0x7ffc2a000000, 0x7ffc2d800000) (58720256 bytes)
[DEBUG] b_bytes range: [0x7ffc65500000, 0x7ffc65600000) (1048576 bytes)
[DEBUG] sfa_bytes range: [0x7ffc28000000, 0x7ffc28700000) (7340032 bytes)
[DEBUG] sfb_bytes range: [0x7ffc5fb83800, 0x7ffc5fba3800) (131072 bytes)
[DEBUG] a shape=(1, 7168, 8192), stride=(58720256, 8192, 1), data_ptr=0x7ffc2a000000
[DEBUG] b shape=(1, 128, 8192), stride=(1048576, 8192, 1), data_ptr=0x7ffc65500000
âœ…(Python) a_bytes 128-byte alignment check passed: 0x7ffc2a000000
âœ…(Python) b_bytes 128-byte alignment check passed: 0x7ffc65500000
LAUNCH DEBUG: M=7168 K=16384 L=1
LAUNCH DEBUG: using fp4_gemv_rank2_cta

[SCALE DEBUG] sfa_ref_cpu shape=torch.Size([7168, 1024, 1]), device=cuda:0
[SCALE DEBUG] sfb_ref_cpu shape=torch.Size([128, 1024, 1]), device=cuda:0
[DEBUG] No padding needed, K_scales=1024
[DEBUG] a_bytes: shape=(1, 7168, 8192), stride=(58720256, 8192, 1), elem_size=1, numel=58720256, bytes=58720256, data_ptr=0x7ffc2a000000
[DEBUG] b_bytes: shape=(1, 128, 8192), stride=(1048576, 8192, 1), elem_size=1, numel=1048576, bytes=1048576, data_ptr=0x7ffc65500000
[DEBUG] sfa_bytes: shape=(1, 7168, 1024), stride=(7340032, 1024, 1), elem_size=1, numel=7340032, bytes=7340032, data_ptr=0x7ffc28000000
[DEBUG] sfb_bytes: shape=(1, 128, 1024), stride=(131072, 1024, 1), elem_size=1, numel=131072, bytes=131072, data_ptr=0x7ffc5fb83800
[DEBUG] sfa_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 0, 64, 64, 0, 64, 64, 56]
[DEBUG] sfa_bytes[0,1,:8] (batch 0, row 1, first 8 scales): [0, 56, 56, 0, 64, 56, 56, 64]
[DEBUG] sfa_bytes[0,2,:8] (batch 0, row 2, first 8 scales): [0, 64, 64, 56, 0, 64, 56, 0]
[DEBUG] sfa_bytes[0,3,:8] (batch 0, row 3, first 8 scales): [64, 64, 0, 56, 56, 56, 56, 56]
[DEBUG] sfa_bytes[0,4,:8] (batch 0, row 4, first 8 scales): [56, 56, 64, 64, 0, 0, 0, 64]
[DEBUG] sfb_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 56, 56, 0, 64, 64, 64, 56]
[DEBUG] a_bytes range: [0x7ffc2a000000, 0x7ffc2d800000) (58720256 bytes)
[DEBUG] b_bytes range: [0x7ffc65500000, 0x7ffc65600000) (1048576 bytes)
[DEBUG] sfa_bytes range: [0x7ffc28000000, 0x7ffc28700000) (7340032 bytes)
[DEBUG] sfb_bytes range: [0x7ffc5fb83800, 0x7ffc5fba3800) (131072 bytes)
[DEBUG] a shape=(1, 7168, 8192), stride=(58720256, 8192, 1), data_ptr=0x7ffc2a000000
[DEBUG] b shape=(1, 128, 8192), stride=(1048576, 8192, 1), data_ptr=0x7ffc65500000
âœ…(Python) a_bytes 128-byte alignment check passed: 0x7ffc2a000000
âœ…(Python) b_bytes 128-byte alignment check passed: 0x7ffc65500000
LAUNCH DEBUG: M=7168 K=16384 L=1
LAUNCH DEBUG: using fp4_gemv_rank2_cta

[SCALE DEBUG] sfa_ref_cpu shape=torch.Size([7168, 1024, 1]), device=cuda:0
[SCALE DEBUG] sfb_ref_cpu shape=torch.Size([128, 1024, 1]), device=cuda:0
[DEBUG] No padding needed, K_scales=1024
[DEBUG] a_bytes: shape=(1, 7168, 8192), stride=(58720256, 8192, 1), elem_size=1, numel=58720256, bytes=58720256, data_ptr=0x7ffc2a000000
[DEBUG] b_bytes: shape=(1, 128, 8192), stride=(1048576, 8192, 1), elem_size=1, numel=1048576, bytes=1048576, data_ptr=0x7ffc65500000
[DEBUG] sfa_bytes: shape=(1, 7168, 1024), stride=(7340032, 1024, 1), elem_size=1, numel=7340032, bytes=7340032, data_ptr=0x7ffc28000000
[DEBUG] sfb_bytes: shape=(1, 128, 1024), stride=(131072, 1024, 1), elem_size=1, numel=131072, bytes=131072, data_ptr=0x7ffc5fb83800
[DEBUG] sfa_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 0, 64, 64, 0, 64, 64, 56]
[DEBUG] sfa_bytes[0,1,:8] (batch 0, row 1, first 8 scales): [0, 56, 56, 0, 64, 56, 56, 64]
[DEBUG] sfa_bytes[0,2,:8] (batch 0, row 2, first 8 scales): [0, 64, 64, 56, 0, 64, 56, 0]
[DEBUG] sfa_bytes[0,3,:8] (batch 0, row 3, first 8 scales): [64, 64, 0, 56, 56, 56, 56, 56]
[DEBUG] sfa_bytes[0,4,:8] (batch 0, row 4, first 8 scales): [56, 56, 64, 64, 0, 0, 0, 64]
[DEBUG] sfb_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 56, 56, 0, 64, 64, 64, 56]
[DEBUG] a_bytes range: [0x7ffc2a000000, 0x7ffc2d800000) (58720256 bytes)
[DEBUG] b_bytes range: [0x7ffc65500000, 0x7ffc65600000) (1048576 bytes)
[DEBUG] sfa_bytes range: [0x7ffc28000000, 0x7ffc28700000) (7340032 bytes)
[DEBUG] sfb_bytes range: [0x7ffc5fb83800, 0x7ffc5fba3800) (131072 bytes)
[DEBUG] a shape=(1, 7168, 8192), stride=(58720256, 8192, 1), data_ptr=0x7ffc2a000000
[DEBUG] b shape=(1, 128, 8192), stride=(1048576, 8192, 1), data_ptr=0x7ffc65500000
âœ…(Python) a_bytes 128-byte alignment check passed: 0x7ffc2a000000
âœ…(Python) b_bytes 128-byte alignment check passed: 0x7ffc65500000
LAUNCH DEBUG: M=7168 K=16384 L=1
LAUNCH DEBUG: using fp4_gemv_rank2_cta
âœ“ Kernel executed successfully! Output shape: torch.Size([7168, 1, 1])
  Runs: 200
  Mean: 43385974.45 ns (43385.97 Î¼s, 43.386 ms)
  Std:  3022068.66 ns (3022.07 Î¼s)
  Err:  213692.52 ns (213.69 Î¼s)
  Best: 35468799.59 ns (35468.80 Î¼s, 35.469 ms)
  Worst: 46169952.39 ns (46169.95 Î¼s, 46.170 ms)
  Relative error: 0.493%
  ðŸ“Š vs Speed of Light: 43385.97 Î¼s / 8.622 Î¼s = 5032.01x slower
     Best: 35468.80 Î¼s / 8.622 Î¼s = 4113.76x slower

Test: M=4096, K=7168, L=8
Config: rank-3: Cluster + SWIZZLE_128B + box_k=K_scales_padded
Speed of Light Target: 17.275 Î¼s

[SCALE DEBUG] sfa_ref_cpu shape=torch.Size([4096, 448, 8]), device=cuda:0
[SCALE DEBUG] sfb_ref_cpu shape=torch.Size([128, 448, 8]), device=cuda:0
[DEBUG] No padding needed, K_scales=448
[DEBUG] a_bytes: shape=(8, 4096, 3584), stride=(14680064, 3584, 1), elem_size=1, numel=117440512, bytes=117440512, data_ptr=0x7ffb27000000
[DEBUG] b_bytes: shape=(8, 128, 3584), stride=(458752, 3584, 1), elem_size=1, numel=3670016, bytes=3670016, data_ptr=0x7ffc7ee00000
[DEBUG] sfa_bytes: shape=(8, 4096, 448), stride=(1835008, 448, 1), elem_size=1, numel=14680064, bytes=14680064, data_ptr=0x7ffc64000000
[DEBUG] sfb_bytes: shape=(8, 128, 448), stride=(57344, 448, 1), elem_size=1, numel=458752, bytes=458752, data_ptr=0x7ffc5fb83800
[DEBUG] sfa_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [64, 0, 0, 56, 0, 0, 56, 0]
[DEBUG] sfa_bytes[0,1,:8] (batch 0, row 1, first 8 scales): [56, 0, 56, 0, 0, 56, 0, 0]
[DEBUG] sfa_bytes[0,2,:8] (batch 0, row 2, first 8 scales): [56, 56, 56, 56, 56, 56, 0, 0]
[DEBUG] sfa_bytes[0,3,:8] (batch 0, row 3, first 8 scales): [64, 64, 64, 56, 56, 0, 56, 0]
[DEBUG] sfa_bytes[0,4,:8] (batch 0, row 4, first 8 scales): [64, 64, 64, 56, 64, 0, 56, 0]
[DEBUG] sfb_bytes[0,0,:8] (batch 0, row 0, first 8 scales): [56, 0, 64, 56, 0, 56, 64, 0]
[DEBUG] a_bytes range: [0x7ffb27000000, 0x7ffb2e000000) (117440512 bytes)
[DEBUG] b_bytes range: [0x7ffc7ee00000, 0x7ffc7f180000) (3670016 bytes)
[DEBUG] sfa_bytes range: [0x7ffc64000000, 0x7ffc64e00000) (14680064 bytes)
[DEBUG] sfb_bytes range: [0x7ffc5fb83800, 0x7ffc5fbf3800) (458752 bytes)
[DEBUG] a shape=(8, 4096, 3584), stride=(14680064, 3584, 1), data_ptr=0x7ffb27000000
[DEBUG] b shape=(8, 128, 3584), stride=(458752, 3584, 1), data_ptr=0x7ffc7ee00000
âœ…(Python) a_bytes 128-byte alignment check passed: 0x7ffb27000000
âœ…(Python) b_bytes 128-byte alignment check passed: 0x7ffc7ee00000
LAUNCH DEBUG: M=4096 K=7168 L=8
LAUNCH DEBUG: using fp4_gemv_rank3_cluster
Device limits:
  sharedMemPerBlock: 49152 (48.0 KB)
  sharedMemPerBlockOptin: 232448 (227.0 KB)
  maxBlocksPerMultiProcessor: 32
  clusterDimSupported: 1
  Requested shared_bytes: 189440 (185.0 KB)
Launch config verification:
  kernel_ptr=0x7ffea80a8cd0
  grid=(32,8,1)
  block=(320,1,1)
  shared_bytes=189440
âœ“ Rank-3 L4, L8 kernel launched successfully!
^C
Thread 1 "python3" received signal SIGINT, Interrupt.
[Switching focus to CUDA kernel 3, grid 1737, cluster (0,0,0), block (0,0,0), thread (0,0,0), device 0, sm 142, warp 0, lane 0]
0x00007ffea15f9820 in void fp4_gemv_rank3_cluster<128, 256, 320>(unsigned char const*, unsigned char const*, unsigned char const*, unsigned char const*, CUtensorMap_st const*, CUtensorMap_st const*, CUtensorMap_st const*, CUtensorMap_st const*, __half*, int, int, int, int)<<<(32,8,1),(320,1,1)>>> ()
(cuda-gdb) info cuda threads       
  BlockIdx ThreadIdx To BlockIdx To ThreadIdx Count                 PC Filename  Line 
Kernel 3
*  (0,0,0)   (0,0,0)     (0,0,0)    (319,0,0)   320 0x00007ffea15f9820              0 
   (1,0,0)   (0,0,0)     (1,0,0)    (319,0,0)   320 0x00007ffea15f6060              0 
   (2,0,0)   (0,0,0)     (2,0,0)    (319,0,0)   320 0x00007ffea15f9820              0 
   (3,0,0)   (0,0,0)     (3,0,0)    (319,0,0)   320 0x00007ffea15f6060              0 
   (4,0,0)   (0,0,0)     (4,0,0)    (319,0,0)   320 0x00007ffea15f9820              0 
   (5,0,0)   (0,0,0)     (5,0,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (14,0,0)   (0,0,0)    (14,0,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (15,0,0)   (0,0,0)    (15,0,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (20,0,0)   (0,0,0)    (20,0,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (21,0,0)   (0,0,0)    (21,0,0)    (319,0,0)   320 0x00007ffea15f6060              0 
   (0,1,0)   (0,0,0)     (0,1,0)    (319,0,0)   320 0x00007ffea15f9820              0 
   (1,1,0)   (0,0,0)     (1,1,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (10,2,0)   (0,0,0)    (10,2,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (11,2,0)   (0,0,0)    (11,2,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (16,2,0)   (0,0,0)    (16,2,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (17,2,0)   (0,0,0)    (17,2,0)    (319,0,0)   320 0x00007ffea15f6060              0 
   (0,3,0)   (0,0,0)     (0,3,0)    (319,0,0)   320 0x00007ffea15f9820              0 
   (1,3,0)   (0,0,0)     (1,3,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (12,3,0)   (0,0,0)    (12,3,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (13,3,0)   (0,0,0)    (13,3,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (14,3,0)   (0,0,0)    (14,3,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (15,3,0)   (0,0,0)    (15,3,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (18,3,0)   (0,0,0)    (18,3,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (19,3,0)   (0,0,0)    (19,3,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (22,3,0)   (0,0,0)    (22,3,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (23,3,0)   (0,0,0)    (23,3,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (24,3,0)   (0,0,0)    (24,3,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (25,3,0)   (0,0,0)    (25,3,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (26,3,0)   (0,0,0)    (26,3,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (27,3,0)   (0,0,0)    (27,3,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (28,3,0)   (0,0,0)    (28,3,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (29,3,0)   (0,0,0)    (29,3,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (20,4,0)   (0,0,0)    (20,4,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (21,4,0)   (0,0,0)    (21,4,0)    (319,0,0)   320 0x00007ffea15f6060              0 
   (0,5,0)   (0,0,0)     (0,5,0)    (319,0,0)   320 0x00007ffea15f9820              0 
   (1,5,0)   (0,0,0)     (1,5,0)    (319,0,0)   320 0x00007ffea15f6060              0 
   (4,5,0)   (0,0,0)     (4,5,0)    (319,0,0)   320 0x00007ffea15f9820              0 
   (5,5,0)   (0,0,0)     (5,5,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (18,5,0)   (0,0,0)    (18,5,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (19,5,0)   (0,0,0)    (19,5,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (22,5,0)   (0,0,0)    (22,5,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (23,5,0)   (0,0,0)    (23,5,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (26,5,0)   (0,0,0)    (26,5,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (27,5,0)   (0,0,0)    (27,5,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (28,5,0)   (0,0,0)    (28,5,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (29,5,0)   (0,0,0)    (29,5,0)    (319,0,0)   320 0x00007ffea15f6060              0 
   (0,6,0)   (0,0,0)     (0,6,0)    (319,0,0)   320 0x00007ffea15f9820              0 
   (1,6,0)   (0,0,0)     (1,6,0)    (319,0,0)   320 0x00007ffea15f6060              0 
   (2,6,0)   (0,0,0)     (2,6,0)    (319,0,0)   320 0x00007ffea15f9820              0 
   (3,6,0)   (0,0,0)     (3,6,0)    (319,0,0)   320 0x00007ffea15f6060              0 
   (4,6,0)   (0,0,0)     (4,6,0)    (319,0,0)   320 0x00007ffea15f9820              0 
   (5,6,0)   (0,0,0)     (5,6,0)    (319,0,0)   320 0x00007ffea15f6060              0 
   (6,6,0)   (0,0,0)     (6,6,0)    (319,0,0)   320 0x00007ffea15f9820              0 
   (7,6,0)   (0,0,0)     (7,6,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (12,6,0)   (0,0,0)    (12,6,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (13,6,0)   (0,0,0)    (13,6,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (10,7,0)   (0,0,0)    (10,7,0)    (319,0,0)   320 0x00007ffea15f9820              0 
--Type <RET> for more, q to quit, c to continue without paging--c 
  (11,7,0)   (0,0,0)    (11,7,0)    (319,0,0)   320 0x00007ffea15f6060              0 
  (30,7,0)   (0,0,0)    (30,7,0)    (319,0,0)   320 0x00007ffea15f9820              0 
  (31,7,0)   (0,0,0)    (31,7,0)    (319,0,0)   320 0x00007ffea15f6060              0 
(cuda-gdb) cuda kernel block thread
kernel 3, block (0,0,0), thread (0,0,0)
(cuda-gdb) backtrace               
#0  0x00007ffea15f9820 in void fp4_gemv_rank3_cluster<128, 256, 320>(unsigned char const*, unsigned char const*, unsigned char const*, unsigned char const*, CUtensorMap_st const*, CUtensorMap_st const*, CUtensorMap_st const*, CUtensorMap_st const*, __half*, int, int, int, int)<<<(32,8,1),(320,1,1)>>> ()
(cuda-gdb) thread apply all bt     

Thread 33 (Thread 0x7ffe9c9e3640 (LWP 29839) "cuda-EvtHandlr"):
#0  0x00007ffff7d6dc3f in poll () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007ffee96d3e27 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#2  0x00007ffee97c35f7 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#3  0x00007ffee96bfbb3 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#4  0x00007ffff7ce9ac3 in ?? () from /lib/x86_64-linux-gnu/libc.so.6
#5  0x00007ffff7d7b8c0 in ?? () from /lib/x86_64-linux-gnu/libc.so.6

Thread 32 (Thread 0x7ffe93fff640 (LWP 29830) "python3"):
#0  0x00007ffff7d7ae9e in epoll_wait () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007ffe9883a796 in ?? () from /lib/x86_64-linux-gnu/libcudadebugger.so.1
#2  0x00007ffe98837cf9 in ?? () from /lib/x86_64-linux-gnu/libcudadebugger.so.1
#3  0x00007ffe98839d5a in ?? () from /lib/x86_64-linux-gnu/libcudadebugger.so.1
#4  0x00007ffe9884488a in ?? () from /lib/x86_64-linux-gnu/libcudadebugger.so.1
#5  0x00007ffe98844d9b in ?? () from /lib/x86_64-linux-gnu/libcudadebugger.so.1
#6  0x00007ffe987fd90d in ?? () from /lib/x86_64-linux-gnu/libcudadebugger.so.1
#7  0x00007ffe98a52dfd in ?? () from /lib/x86_64-linux-gnu/libcudadebugger.so.1
#8  0x00007ffff7ce9ac3 in ?? () from /lib/x86_64-linux-gnu/libc.so.6
#9  0x00007ffff7d7b8c0 in ?? () from /lib/x86_64-linux-gnu/libc.so.6

Thread 31 (Thread 0x7ffe995dc640 (LWP 29829) "cuda00001400006"):
#0  0x00007ffff7d6dc3f in poll () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007ffee96d3e27 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#2  0x00007ffee97c35f7 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#3  0x00007ffee96bfbb3 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#4  0x00007ffff7ce9ac3 in ?? () from /lib/x86_64-linux-gnu/libc.so.6
#5  0x00007ffff7d7b8c0 in ?? () from /lib/x86_64-linux-gnu/libc.so.6

Thread 1 (Thread 0x7ffff7c54000 (LWP 29798) "python3"):
#0  0x00007ffee95d6104 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#1  0x00007ffee9614e3b in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#2  0x00007ffeea354137 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#3  0x00007ffeea354585 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#4  0x00007ffee95e29f4 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#5  0x00007ffee9608ce2 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#6  0x00007ffee96145d5 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#7  0x00007ffee95614f9 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#8  0x00007ffee96f0f77 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1
#9  0x00007ffee96e1520 in cuCtxSynchronize () from /lib/x86_64-linux-gnu/libcuda.so.1
#10 0x00007ffff6e0fb8b in ?? () from /root/reference-kernels/problems/nvidia/nvfp4_gemv/.venv/lib/python3.10/site-packages/torch/lib/../../nvidia/cuda_runtime/lib/libcudart.so.12
#11 0x00007ffff6e4a43a in cudaDeviceSynchronize () from /root/reference-kernels/problems/nvidia/nvfp4_gemv/.venv/lib/python3.10/site-packages/torch/lib/../../nvidia/cuda_runtime/lib/libcudart.so.12
#12 0x00007ffea80a846d in launch_fp4_gemv_optimized(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long) () from /root/.cache/torch_extensions/py310_cu128/nvfp4_gemv_sm100_ptx/nvfp4_gemv_sm100_ptx.so
#13 0x00007ffea8098155 in void std::__invoke_impl<void, void (* const&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long>(std::__invoke_other, void (* const&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), at::Tensor&&, at::Tensor&&, at::Tensor&&, at::Tensor&&, at::Tensor&&, long&&, long&&, long&&, long&&) () from /root/.cache/torch_extensions/py310_cu128/nvfp4_gemv_sm100_ptx/nvfp4_gemv_sm100_ptx.so
#14 0x00007ffea8091d32 in std::__invoke_result<void (* const&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long>::type std::__invoke<void (* const&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long>(void (* const&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), at::Tensor&&, at::Tensor&&, at::Tensor&&, at::Tensor&&, at::Tensor&&, long&&, long&&, long&&, long&&) () from /root/.cache/torch_extensions/py310_cu128/nvfp4_gemv_sm100_ptx/nvfp4_gemv_sm100_ptx.so
#15 0x00007ffea808a631 in std::invoke_result<void (* const&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long>::type std::invoke<void (* const&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long>(void (* const&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), at::Tensor&&, at::Tensor&&, at::Tensor&&, at::Tensor&&, at::Tensor&&, long&&, long&&, long&&, long&&) () from /root/.cache/torch_extensions/py310_cu128/nvfp4_gemv_sm100_ptx/nvfp4_gemv_sm100_ptx.so
#16 0x00007ffea8082bc1 in torch::detail::wrap_pybind_function_impl_<void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul, false>(void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), std::integer_sequence<unsigned long, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul>, std::integral_constant<bool, false>)::{lambda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long)#1}::operator()(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long) const () from /root/.cache/torch_extensions/py310_cu128/nvfp4_gemv_sm100_ptx/nvfp4_gemv_sm100_ptx.so
#17 0x00007ffea80a2b83 in void pybind11::detail::argument_loader<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long>::call_impl<void, torch::detail::wrap_pybind_function_impl_<void (&)(at::Tensor, at::T--Type <RET> for more, q to quit, c to continue without paging--c
ensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul, false>(void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), std::integer_sequence<unsigned long, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul>, std::integral_constant<bool, false>)::{lambda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long)#1}&, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul, pybind11::detail::void_type>(torch::detail::wrap_pybind_function_impl_<void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul, false>(void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), std::integer_sequence<unsigned long, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul>, std::integral_constant<bool, false>)::{lambda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long)#1}&, std::integer_sequence<unsigned long, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul>, pybind11::detail::void_type&&) && () from /root/.cache/torch_extensions/py310_cu128/nvfp4_gemv_sm100_ptx/nvfp4_gemv_sm100_ptx.so
#18 0x00007ffea809e10c in std::enable_if<std::is_void<void>::value, pybind11::detail::void_type>::type pybind11::detail::argument_loader<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long>::call<void, pybind11::detail::void_type, torch::detail::wrap_pybind_function_impl_<void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul, false>(void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), std::integer_sequence<unsigned long, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul>, std::integral_constant<bool, false>)::{lambda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long)#1}&>(torch::detail::wrap_pybind_function_impl_<void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul, false>(void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), std::integer_sequence<unsigned long, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul>, std::integral_constant<bool, false>)::{lambda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long)#1}&) && () from /root/.cache/torch_extensions/py310_cu128/nvfp4_gemv_sm100_ptx/nvfp4_gemv_sm100_ptx.so
#19 0x00007ffea8098362 in pybind11::cpp_function::initialize<torch::detail::wrap_pybind_function_impl_<void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul, false>(void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), std::integer_sequence<unsigned long, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul>, std::integral_constant<bool, false>)::{lambda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long)#1}, void, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long, pybind11::name, pybind11::scope, pybind11::sibling, char [26]>(void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), void (*)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, char const (&) [26])::{lambda(pybind11::detail::function_call&)#3}::operator()(pybind11::detail::function_call&) const () from /root/.cache/torch_extensions/py310_cu128/nvfp4_gemv_sm100_ptx/nvfp4_gemv_sm100_ptx.so
#20 0x00007ffea809887c in pybind11::cpp_function::initialize<torch::detail::wrap_pybind_function_impl_<void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul, false>(void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), std::integer_sequence<unsigned long, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul>, std::integral_constant<bool, false>)::{lambda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long)#1}, void, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long, pybind11::name, pybind11::scope, pybind11::sibling, char [26]>(void (&)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), void (*)(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, long, long, long, long), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, char const (&) [26])::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) () from /root/.cache/torch_extensions/py310_cu128/nvfp4_gemv_sm100_ptx/nvfp4_gemv_sm100_ptx.so
#21 0x00007ffea80809a2 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from /root/.cache/torch_extensions/py310_cu128/nvfp4_gemv_sm100_ptx/nvfp4_gemv_sm100_ptx.so
#22 0x00005555556de852 in ?? ()
#23 0x00005555556d512b in _PyObject_MakeTpCall ()
#24 0x00005555556ced96 in _PyEval_EvalFrameDefault ()
#25 0x00005555556df0ac in _PyFunction_Vectorcall ()
#26 0x00005555556c9460 in _PyEval_EvalFrameDefault ()
#27 0x00005555557adbe6 in ?? ()
#28 0x00005555557adab6 in PyEval_EvalCode ()
#29 0x00005555557d4528 in ?? ()
#30 0x00005555557ceb7f in ?? ()
#31 0x00005555557d42c5 in ?? ()
#32 0x00005555557d3808 in _PyRun_SimpleFileObject ()
#33 0x00005555557d34e7 in _PyRun_AnyFileObject ()
#34 0x00005555557c7a8e in Py_RunMain ()
#35 0x00005555557a1a8d in Py_BytesMain ()
#36 0x00007ffff7c7ed90 in ?? () from /lib/x86_64-linux-gnu/libc.so.6
#37 0x00007ffff7c7ee40 in __libc_start_main () from /lib/x86_64-linux-gnu/libc.so.6
#38 0x00005555557a1985 in _start ()
(cuda-gdb) 