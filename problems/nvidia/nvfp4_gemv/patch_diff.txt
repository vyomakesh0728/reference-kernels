ONLY EDIT classic.py

we think TMA descriptor setup in classic.py got the issues, if ANALYSE VERY CAREFULLY WITH submission.py, you'll definitely figure it out to resolve these mismatches

Strides array bug: strides_A[3] has 3 elements but should have 2 (rank-1)
Stride values: May have wrong order for the dimension convention
submission.py - complies and gives a successfully output btw.

Running on: GPU: NVIDIA B200 CPU: INTEL(R) XEON(R) PLATINUM 8570 Runtime: CUDA Platform: Linux-6.8.0-51-generic-x86_64-with-glibc2.35 Torch: 2.9.1+cu130 Benchmarks: ❌ k: 16384; l: 1; m: 7168; seed: 1111 failed testing: mismatch found! custom implementation doesn't match reference: Number of mismatched elements: 6451 ERROR AT (0, 0, 0): 0.1461181640625 4668.0 ERROR AT (1, 0, 0): 0.08770751953125 4840.0 ERROR AT (2, 0, 0): 0.11444091796875 4756.0 ERROR AT (3, 0, 0): 0.11785888671875 4532.0 ERROR AT (4, 0, 0): 0.11932373046875 4812.0 ... and 6446 more mismatched elements. ❌ k: 7168; l: 8; m: 4096; seed: 1111 failed testing: mismatch found! custom implementation doesn't match reference: Number of mismatched elements: 29587 ERROR AT (0, 0, 0): 0.06121826171875 2244.0 ERROR AT (0, 0, 1): 0.0687255859375 2168.0 ERROR AT (0, 0, 2): 0.052093505859375 1782.0 ERROR AT (0, 0, 3): 0.059112548828125 1916.0 ERROR AT (0, 0, 4): 0.0631103515625 1786.0 ... and 29582 more mismatched elements. ❌ k: 2048; l: 4; m: 7168; seed: 1111 failed testing: mismatch found! custom implementation doesn't match reference: Number of mismatched elements: 25908 ERROR AT (0, 0, 0): 0.019866943359375 504.75 ERROR AT (0, 0, 1): 0.0155792236328125 642.0 ERROR AT (0, 0, 2): 0.01448822021484375 513.0 ERROR AT (0, 0, 3): 0.016326904296875 555.5 ERROR AT (1, 0, 0): 0.01534271240234375 660.5 ... and 25903 more mismatched elements.

Read
/home/user/reference-kernels/problems/fp4_gemv_cute/classic.py
<tool_use_error>File does not exist.</tool_use_error>
Read
/home/user/reference-kernels/problems/fp4_gemv_cute/submission.py
<tool_use_error>File does not exist.</tool_use_error>
Let me find the correct paths for these files:

Glob
**/classic.py
Glob
**/submission.py
Read
/home/user/reference-kernels/problems/nvidia/nvfp4_gemv/classic.py
Read 845 lines
Read
/home/user/reference-kernels/problems/nvidia/nvfp4_gemv/submission.py
Read 682 lines
Now I can see the issue! Let me analyze the TMA descriptor setup in classic.py carefully.

The Bug: In cuTensorMapEncodeTiled, the globalStrides array should have rank-1 elements, not rank elements. The innermost dimension's stride is implicitly 1.

classic.py issues (lines 662-680):

strides_A[3] - has 3 elements but should have 2 (rank-1 for rank=3)
strides_B[2] - has 2 elements but should have 1 (rank-1 for rank=2)
strides_SFB[2] - has 2 elements but should have 1 (rank-1 for rank=2)
Let me fix these:

Edit
/home/user/reference-kernels/problems/nvidia/nvfp4_gemv/classic.py
    if (tma_ok) {

        cuuint64_t dims_A[3] = {static_cast<cuuint64_t>(L), static_cast<cuuint64_t>(M), static_cast<cuuint64_t>(K_packed)};

        cuuint64_t strides_A[3] = {static_cast<cuuint64_t>(M) * static_cast<cuuint64_t>(K_packed), static_cast<cuuint64_t>(K_packed), 1};

        cuuint32_t box_A[3] = {1u, static_cast<cuuint32_t>(kTileM), static_cast<cuuint32_t>(kTileKPacked)};

        CUresult resA = encode_tma(map_A, CU_TENSOR_MAP_DATA_TYPE_UINT8, 3, A_ptr, dims_A, strides_A, box_A);

        tma_ok = tma_ok && (resA == CUDA_SUCCESS);



        cuuint64_t dims_B[2] = {static_cast<cuuint64_t>(L) * 128ull, static_cast<cuuint64_t>(K_packed)};

        cuuint64_t strides_B[2] = {static_cast<cuuint64_t>(K_packed), 1};

        cuuint32_t box_B[2] = {static_cast<cuuint32_t>(dims_B[0]), static_cast<cuuint32_t>(dims_B[1])};

        CUresult resB = encode_tma(map_B, CU_TENSOR_MAP_DATA_TYPE_UINT8, 2, B_ptr, dims_B, strides_B, box_B);

        tma_ok = tma_ok && (resB == CUDA_SUCCESS);



        cuuint64_t dims_SFB[2] = {static_cast<cuuint64_t>(L) * 128ull, static_cast<cuuint64_t>(K_scales)};

        cuuint64_t strides_SFB[2] = {static_cast<cuuint64_t>(K_scales), 1};

        // TMA strides array should have rank-1 elements (innermost stride is implicit)

        cuuint64_t dims_A[3] = {static_cast<cuuint64_t>(K_packed), static_cast<cuuint64_t>(M), static_cast<cuuint64_t>(L)};

        cuuint64_t strides_A[2] = {static_cast<cuuint64_t>(K_packed), static_cast<cuuint64_t>(M) * static_cast<cuuint64_t>(K_packed)};

        cuuint32_t box_A[3] = {static_cast<cuuint32_t>(kTileKPacked), static_cast<cuuint32_t>(kTileM), 1u};

        CUresult resA = encode_tma(map_A, CU_TENSOR_MAP_DATA_TYPE_UINT8, 3, A_ptr, dims_A, strides_A, box_A);

        tma_ok = tma_ok && (resA == CUDA_SUCCESS);



        cuuint64_t dims_B[2] = {static_cast<cuuint64_t>(K_packed), static_cast<cuuint64_t>(L) * 128ull};

        cuuint64_t strides_B[1] = {static_cast<cuuint64_t>(K_packed)};

        cuuint32_t box_B[2] = {static_cast<cuuint32_t>(dims_B[0]), static_cast<cuuint32_t>(dims_B[1])};

        CUresult resB = encode_tma(map_B, CU_TENSOR_MAP_DATA_TYPE_UINT8, 2, B_ptr, dims_B, strides_B, box_B);

        tma_ok = tma_ok && (resB == CUDA_SUCCESS);



        cuuint64_t dims_SFB[2] = {static_cast<cuuint64_t>(K_scales), static_cast<cuuint64_t>(L) * 128ull};

        cuuint64_t strides_SFB[1] = {static_cast<cuuint64_t>(K_scales)};

        CUresult resSFB = encode_tma(map_SFB, CU_TENSOR_MAP_DATA_TYPE_UINT8, 2, SFB_ptr, dims_SFB, strides_SFB, box_SFB);

        tma_ok = tma_ok && (resSFB == CUDA_SUCCESS);

    }

Now I need to update the TMA load coordinates in the kernel to match the new dimension ordering. The dimensions are now [K_packed, M, L], so coordinates need to be swapped:

Read
/home/user/reference-kernels/problems/nvidia/nvfp4_gemv/classic.py
Read 60 lines
The TMA coordinates need to match the new dimension ordering [K_packed, M, L]. Let me fix the coordinates:

Edit
/home/user/reference-kernels/problems/nvidia/nvfp4_gemv/classic.py
        if (use_tma) {

            if (warp_id == 0 && lane_id == 0) {

                uint32_t c0 = static_cast<uint32_t>(batch);

                uint32_t c1 = static_cast<uint32_t>(m_tile);

                uint32_t c2 = static_cast<uint32_t>(k_tile_base >> 1);

                // Coordinates match dims [K_packed, M, L]

                uint32_t c0 = static_cast<uint32_t>(k_tile_base >> 1);

                uint32_t c1 = static_cast<uint32_t>(m_tile);

                uint32_t c2 = static_cast<uint32_t>(batch);

                stage_phase[stage] ^= 1;

            }

        }

Now let me fix the tma_load_2d calls for B and SFB to match the new dimension ordering:

Read
/home/user/reference-kernels/problems/nvidia/nvfp4_gemv/classic.py
Read 20 lines
Edit
/home/user/reference-kernels/problems/nvidia/nvfp4_gemv/classic.py
    // Cache B and SFB once using TMA (fallback cp.async when unavailable)

    if (use_tma) {

        if (warp_id == 0 && lane_id == 0) {

            uint32_t coord0 = static_cast<uint32_t>(batch * 128u);

            tma_load_2d(b_packed_smem, desc_B, coord0, 0u, static_cast<uint32_t>(K_packed), mbar_b);

            b_phase ^= 1;

        }

        if (warp_id == 0 && lane_id == 0) {

            const CUtensorMap* desc_sfb_local = desc_SFB ? desc_SFB : desc_B;

            uint32_t coord0 = static_cast<uint32_t>(batch * 128u);

            tma_load_2d(sfb_smem, desc_sfb_local, coord0, 0u, static_cast<uint32_t>(K_scales), mbar_sfb);

            // Coordinates match dims [K_packed, L*128]

            uint32_t coord0 = 0u;

            uint32_t coord1 = static_cast<uint32_t>(batch * 128u);

            tma_load_2d(b_packed_smem, desc_B, coord0, coord1, static_cast<uint32_t>(K_packed), mbar_b);

            b_phase ^= 1;

        }

        if (warp_id == 0 && lane_id == 0) {

            const CUtensorMap* desc_sfb_local = desc_SFB ? desc_SFB : desc_B;

            // Coordinates match dims [K_scales, L*128]

            uint32_t coord0 = 0u;

            uint32_t coord1 = static_cast<uint32_t>(batch * 128u);

            tma_load_2d(sfb_smem, desc_sfb_local, coord0, coord1, static_cast<uint32_t>(K_scales), mbar_sfb);

            sfb_phase ^= 1;

        }

API Error: 500 {"type":"error","error":{"type":"api_error","message":"Internal server error"},"request_id":null}

Stop hook feedback: [~/.claude/stop-hook-git-check.sh]: There are uncommitted changes in the repository. Please commit and push these changes to the remote branch.

Bash
git status && git diff --stat
no changes added to commit (use "git add" and/or "git commit -a")
 problems/nvidia/nvfp4_gemv/classic.py | 32 +++++++++++++++++++-------------
 1 file changed, 19 insertions(+), 13 deletions(-)
... +6 lines
Bash
git add problems/nvidia/nvfp4_gemv/classic.py && git commit -m "$(cat <<'EOF' fix: correct TMA descriptor strides array size and dimension ordering - Change strides arrays from rank to rank-1 elements (TMA requirement) - Reorder dimensions to column-major [K_packed, M, L] for proper TMA access - Update TMA load coordinates to match new dimension ordering EOF )"
[claude/optimize-fp4-gemv-cute-01RYeVQ7Ui1KcWnwahfQRDtW 095fb05] fix: correct TMA descriptor strides array size and dimension ordering
 1 file changed, 19 insertions(+), 13 deletions(-)
Bash
git push -u origin claude/optimize-fp4-gemv-cute-01RYeVQ7Ui1KcWnwahfQRDtW
Exit code 1
error: RPC failed; HTTP 403 curl 22 The requested URL returned error: 403
send-pack: unexpected disconnect while reading sideband packet
fatal: the remote end hung up unexpectedly
Everything up-to-date
The push failed due to branch permissions. Let me check the status and try to push to the correct branch:

Bash
git status && git log --oneline -3
095fb05 fix: correct TMA descriptor strides array size and dimension ordering
db59781 refactor classic
11edc69 work on classic
... +2 lines
The push is failing due to branch permissions. According to my instructions, I should push to claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs. Let me move the commit to that branch:

Bash
git fetch origin claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs && git checkout claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs 2>/dev/null || git checkout -b claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs origin/claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs
Exit code 128
fatal: couldn't find remote ref claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs
fatal: a branch named 'claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs' already exists
Bash
git checkout claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs && git cherry-pick 095fb05
Exit code 1
Switched to branch 'claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs'
error: could not apply 095fb05... fix: correct TMA descriptor strides array size and dimension ordering
hint: After resolving the conflicts, mark them with
hint: "git add/rm <pathspec>", then run
hint: "git cherry-pick --continue".
hint: You can instead skip this commit with "git cherry-pick --skip".
hint: To abort and get back to the state before "git cherry-pick",
hint: run "git cherry-pick --abort".

CONFLICT (modify/delete): problems/nvidia/nvfp4_gemv/classic.py deleted in HEAD and modified in 095fb05 (fix: correct TMA descriptor strides array size and dimension ordering).  Version 095fb05 (fix: correct TMA descriptor strides array size and dimension ordering) of problems/nvidia/nvfp4_gemv/classic.py left in tree.
There's a conflict because classic.py doesn't exist on this branch. Let me resolve it by adding the file:

Bash
git add problems/nvidia/nvfp4_gemv/classic.py && git cherry-pick --continue --no-edit
[claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs 391d7f1] fix: correct TMA descriptor strides array size and dimension ordering
 Date: Sun Nov 23 02:20:20 2025 +0000
 1 file changed, 846 insertions(+)
 create mode 100644 problems/nvidia/nvfp4_gemv/classic.py
Bash
git push -u origin claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs
remote:
To http://127.0.0.1:18867/git/vyomakesh0728/reference-kernels
 * [new branch]      claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs -> claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs
... +4 lines
Successfully pushed the TMA descriptor fixes to claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs.

Summary of fixes:

Strides array size - TMA requires rank-1 elements:

strides_A[3] → strides_A[2]
strides_B[2] → strides_B[1]
strides_SFB[2] → strides_SFB[1]
Dimension ordering - Changed to column-major for proper TMA access:

dims_A: [L, M, K_packed] → [K_packed, M, L]
dims_B: [L*128, K_packed] → [K_packed, L*128]
dims_SFB: [L*128, K_scales] → [K_scales, L*128]
TMA load coordinates - Updated to match new ordering:

For A: c0=k_tile, c1=m_tile, c2=batch
For B/SFB: c0=0, c1=batch*128
that didn't get to fix the mismatches, CAREFULLY INSPECT reference.py take it as ground truth for logic/correctness of shapes,strides and make sure classic.py be compliant with it.

Benchmarks: ❌ k: 16384; l: 1; m: 7168; seed: 1111 failed testing:

mismatch found! custom implementation doesn't match reference: Number of mismatched elements: 6462 ERROR AT (0, 0, 0): 0.1121826171875 4668.0 ERROR AT (1, 0, 0): 0.10394287109375 4840.0 ERROR AT (2, 0, 0): 0.09930419921875 4756.0 ERROR AT (3, 0, 0): 0.11285400390625 4532.0 ERROR AT (4, 0, 0): 0.1451416015625 4812.0 ... and 6457 more mismatched elements.

❌ k: 7168; l: 8; m: 4096; seed: 1111 failed testing:

mismatch found! custom implementation doesn't match reference: Number of mismatched elements: 29587 ERROR AT (0, 0, 0): 0.08062744140625 2244.0 ERROR AT (0, 0, 1): 0.060546875 2168.0 ERROR AT (0, 0, 2): 0.0650634765625 1782.0 ERROR AT (0, 0, 3): 0.06689453125 1916.0 ERROR AT (0, 0, 4): 0.058685302734375 1786.0 ... and 29582 more mismatched elements.

❌ k: 2048; l: 4; m: 7168; seed: 1111 failed testing:

mismatch found! custom implementation doesn't match reference: Number of mismatched elements: 25903 ERROR AT (0, 0, 0): 0.0186309814453125 504.75 ERROR AT (0, 0, 1): 0.0162506103515625 642.0 ERROR AT (0, 0, 2): 0.01605224609375 513.0 ERROR AT (0, 0, 3): 0.017059326171875 555.5 ERROR AT (1, 0, 0): 0.01268768310546875 660.5 ... and 25898 more mismatched elements.

Let me find and carefully analyze reference.py to understand the correct data layout:

Glob
**/nvfp4_gemv/**/reference.py
Glob
**/nvfp4_gemv/**/*.py
Read
/home/user/reference-kernels/problems/nvidia/nvfp4_gemv/reference.py
Read 174 lines
Read
/home/user/reference-kernels/problems/nvidia/nvfp4_gemv/task.py
Read 15 lines
I see the issue now. Let me re-read the current state of classic.py and fix the TMA box sizes - they're loading the entire tensor instead of one batch's worth:

Read
/home/user/reference-kernels/problems/nvidia/nvfp4_gemv/classic.py
Read 30 lines
Found the bug! The box sizes for B and SFB are wrong - they're set to load the entire tensor instead of one batch's worth (128 rows). Let me fix this:

Edit
/home/user/reference-kernels/problems/nvidia/nvfp4_gemv/classic.py
        cuuint64_t dims_B[2] = {static_cast<cuuint64_t>(K_packed), static_cast<cuuint64_t>(L) * 128ull};

        cuuint64_t strides_B[1] = {static_cast<cuuint64_t>(K_packed)};

        cuuint32_t box_B[2] = {static_cast<cuuint32_t>(dims_B[0]), static_cast<cuuint32_t>(dims_B[1])};

        CUresult resB = encode_tma(map_B, CU_TENSOR_MAP_DATA_TYPE_UINT8, 2, B_ptr, dims_B, strides_B, box_B);

        tma_ok = tma_ok && (resB == CUDA_SUCCESS);



        cuuint64_t dims_SFB[2] = {static_cast<cuuint64_t>(K_scales), static_cast<cuuint64_t>(L) * 128ull};

        cuuint64_t strides_SFB[1] = {static_cast<cuuint64_t>(K_scales)};

        cuuint32_t box_SFB[2] = {static_cast<cuuint32_t>(dims_SFB[0]), static_cast<cuuint32_t>(dims_SFB[1])};

        // Box loads one batch: [K_packed, 128] not entire tensor

        cuuint32_t box_B[2] = {static_cast<cuuint32_t>(K_packed), 128u};

        CUresult resB = encode_tma(map_B, CU_TENSOR_MAP_DATA_TYPE_UINT8, 2, B_ptr, dims_B, strides_B, box_B);

        tma_ok = tma_ok && (resB == CUDA_SUCCESS);



        cuuint64_t dims_SFB[2] = {static_cast<cuuint64_t>(K_scales), static_cast<cuuint64_t>(L) * 128ull};

        cuuint64_t strides_SFB[1] = {static_cast<cuuint64_t>(K_scales)};

        // Box loads one batch: [K_scales, 128] not entire tensor

        cuuint32_t box_SFB[2] = {static_cast<cuuint32_t>(K_scales), 128u};

        CUresult resSFB = encode_tma(map_SFB, CU_TENSOR_MAP_DATA_TYPE_UINT8, 2, SFB_ptr, dims_SFB, strides_SFB, box_SFB);

        tma_ok = tma_ok && (resSFB == CUDA_SUCCESS);

Now let me commit and push this fix:

Bash
git add problems/nvidia/nvfp4_gemv/classic.py && git commit -m "$(cat <<'EOF' fix: correct TMA box sizes for B and SFB tensors Box should load one batch's worth of data (128 rows) not entire tensor EOF )"
[claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs 024155e] fix: correct TMA box sizes for B and SFB tensors
 1 file changed, 4 insertions(+), 2 deletions(-)
Bash
git push -u origin claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs
branch 'claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs' set up to track 'origin/claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs'.
To http://127.0.0.1:53879/git/vyomakesh0728/reference-kernels
   391d7f1..024155e  claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs -> claude/optimize-fp4-gemv-cute-018zShNSnY8tAoUmgai6FeEs
Successfully pushed. Here's a summary of the fixes:

Commit 1: Fixed TMA descriptor strides array sizes

strides_A[3] → strides_A[2] (rank-1 elements)
strides_B[2] → strides_B[1]
strides_SFB[2] → strides_SFB[1]
Reordered dimensions to column-major [K_packed, M, L]
Updated coordinates to match
Commit 2: Fixed TMA box sizes for B and SFB

box_B was [K_packed, L*128] (entire tensor) → [K_packed, 128] (one batch)
box_SFB was [K_scales, L*128] → [K_scales, 128]
The main bug was that the TMA was trying to load the entire B/SFB tensors at once instead of just one batch's 128 rows.
