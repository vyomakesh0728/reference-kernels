import os

import torch
from task import input_t, output_t
from torch.utils.cpp_extension import load_inline

cutlass_path = os.environ.get("CUTLASS_PATH", "/usr/local/cutlass")

cuda_source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cuda_fp16.h>

#include "cutlass/cutlass.h"
#include "cutlass/numeric_types.h"

// SM100 FP4 Block-Scaled GEMV with Dynamic Shared Memory
// Uses large tiles (256x512) via extern __shared__ for speed-of-light performance

using ElementA = cutlass::float_e2m1_t;      // FP4 E2M1 for matrix A
using ElementB = cutlass::float_e2m1_t;      // FP4 E2M1 for vector B
using ElementC = cutlass::half_t;            // FP16 for output
using ElementSFA = cutlass::float_e4m3_t;    // FP8 E4M3 scale factors for A
using ElementSFB = cutlass::float_e4m3_t;    // FP8 E4M3 scale factors for B

// Tile sizes: Use large tiles with dynamic shared memory
// TM=256, TK=512 (packed as 256 bytes)
constexpr int kTileM = 256;
constexpr int kTileK = 512;
constexpr int kTileK_packed = kTileK / 2;  // FP4 packed 2 per byte

// Thread block configuration
constexpr int kBlockSize = 256;
constexpr int kWarpsPerBlock = kBlockSize / 32;

// Vectorized loads: 16 bytes = 128 bits
constexpr int kVecSize = 16;

// FP4 E2M1 lookup table for decoding nibbles to float
// E2M1: 1 sign bit, 2 exponent bits, 1 mantissa bit, bias=1
// Positive values: 0, 0.5, 1, 1.5, 2, 3, 4, 6
__device__ const float fp4_e2m1_lut[16] = {
    0.0f,   // 0000: +0
    0.5f,   // 0001: +0.5 (denormal)
    1.0f,   // 0010: +1
    1.5f,   // 0011: +1.5
    2.0f,   // 0100: +2
    3.0f,   // 0101: +3
    4.0f,   // 0110: +4
    6.0f,   // 0111: +6
    -0.0f,  // 1000: -0
    -0.5f,  // 1001: -0.5 (denormal)
    -1.0f,  // 1010: -1
    -1.5f,  // 1011: -1.5
    -2.0f,  // 1100: -2
    -3.0f,  // 1101: -3
    -4.0f,  // 1110: -4
    -6.0f   // 1111: -6
};

__forceinline__ __device__ float decode_fp4_e2m1(uint8_t nibble) {
    return fp4_e2m1_lut[nibble & 0x0F];
}

__global__ void fp4_gemv_dynamic_smem_kernel(
    const uint8_t* __restrict__ A,        // [M, K/2] FP4 matrix (packed)
    const uint8_t* __restrict__ B,        // [128, K/2] FP4 vector (packed, padded to 128)
    const uint8_t* __restrict__ SFA,      // [M, K/16] FP8 scale factors for A
    const uint8_t* __restrict__ SFB,      // [128, K/16] FP8 scale factors for B
    half* __restrict__ D,                 // [M] FP16 output
    int M, int K
) {
    // Dynamic shared memory - partition for A tiles, B tiles, and scale factors
    extern __shared__ uint8_t smem[];

    // Partition shared memory:
    // - A_smem: kTileM * kTileK_packed bytes
    // - B_smem: kTileK_packed bytes (vector)
    // - SFA_smem: kTileM * (kTileK/16) bytes
    // - SFB_smem: (kTileK/16) bytes
    uint8_t* A_smem = smem;
    uint8_t* B_smem = A_smem + kTileM * kTileK_packed;
    uint8_t* SFA_smem = B_smem + kTileK_packed;
    uint8_t* SFB_smem = SFA_smem + kTileM * (kTileK / 16);

    const int tid = threadIdx.x;
    const int M_tile_idx = blockIdx.x;
    const int m_start = M_tile_idx * kTileM;

    // Each thread accumulates results for multiple rows
    constexpr int kRowsPerThread = kTileM / kBlockSize;
    float acc[kRowsPerThread];
    #pragma unroll
    for (int i = 0; i < kRowsPerThread; i++) {
        acc[i] = 0.0f;
    }

    const int K_packed = K / 2;
    const int K_scales = K / 16;

    // Loop over K dimension in tiles
    for (int k_tile = 0; k_tile < K; k_tile += kTileK) {
        const int k_start = k_tile;
        const int k_end = min(k_start + kTileK, K);
        const int k_size = k_end - k_start;
        const int k_packed_start = k_start / 2;
        const int k_packed_size = k_size / 2;

        __syncthreads();

        // Cooperatively load A tile into shared memory
        // Each thread loads multiple elements using vectorized loads
        for (int offset = tid * kVecSize; offset < kTileM * k_packed_size; offset += kBlockSize * kVecSize) {
            const int row = offset / k_packed_size;
            const int col = offset % k_packed_size;
            const int m_idx = m_start + row;

            if (m_idx < M && (k_packed_start + col + kVecSize) <= K_packed) {
                // Vectorized 16-byte load
                *reinterpret_cast<uint4*>(&A_smem[row * k_packed_size + col]) =
                    *reinterpret_cast<const uint4*>(&A[m_idx * K_packed + k_packed_start + col]);
            }
        }

        // Load B tile (single vector, all threads cooperate)
        // B is [128, K_packed], explicitly index row 0
        for (int offset = tid; offset < k_packed_size; offset += kBlockSize) {
            if ((k_packed_start + offset) < K_packed) {
                B_smem[offset] = B[0 * K_packed + k_packed_start + offset];  // Row 0 of padded B
            }
        }

        // Load scale factors
        const int k_scale_start = k_start / 16;
        const int k_scale_size = k_size / 16;

        for (int offset = tid; offset < kTileM * k_scale_size; offset += kBlockSize) {
            const int row = offset / k_scale_size;
            const int col = offset % k_scale_size;
            const int m_idx = m_start + row;

            if (m_idx < M && (k_scale_start + col) < K_scales) {
                SFA_smem[row * k_scale_size + col] = SFA[m_idx * K_scales + k_scale_start + col];
            }
        }

        // Load SFB scale factors - SFB is [128, K_scales], explicitly index row 0
        for (int offset = tid; offset < k_scale_size; offset += kBlockSize) {
            if ((k_scale_start + offset) < K_scales) {
                SFB_smem[offset] = SFB[0 * K_scales + k_scale_start + offset];  // Row 0 of padded SFB
            }
        }

        __syncthreads();

        // Compute: Each thread handles kRowsPerThread rows
        #pragma unroll
        for (int r = 0; r < kRowsPerThread; r++) {
            const int local_row = tid * kRowsPerThread + r;
            if (local_row >= kTileM) continue;

            const int m_idx = m_start + local_row;
            if (m_idx >= M) continue;

            // Accumulate dot product over K tile
            float local_sum = 0.0f;

            // Process 32 FP4 elements (16 bytes packed) at a time
            for (int k = 0; k < k_packed_size; k += 16) {
                // Load 16 packed FP4 values from A (32 FP4 elements)
                // Load 16 packed FP4 values from B (32 FP4 elements)
                // Unpack, multiply with scale factors, accumulate

                // Simple scalar implementation (can be optimized with tensor cores)
                for (int kk = 0; kk < 16 && (k + kk) < k_packed_size; kk++) {
                    uint8_t a_packed = A_smem[local_row * k_packed_size + k + kk];
                    uint8_t b_packed = B_smem[k + kk];

                    // Get scale factors (one per 8 packed bytes = 16 FP4 elements)
                    const int scale_idx = (k + kk) / 8;
                    if (scale_idx < k_scale_size) {
                        float scale_a = __half2float(__float2half_rn(
                            *reinterpret_cast<const cutlass::float_e4m3_t*>(&SFA_smem[local_row * k_scale_size + scale_idx])
                        ));
                        float scale_b = __half2float(__float2half_rn(
                            *reinterpret_cast<const cutlass::float_e4m3_t*>(&SFB_smem[scale_idx])
                        ));

                        // Decode FP4 E2M1 nibbles and apply scale factors
                        // Low nibble (bits 0-3)
                        float a_low = decode_fp4_e2m1(a_packed & 0x0F) * scale_a;
                        float b_low = decode_fp4_e2m1(b_packed & 0x0F) * scale_b;
                        local_sum += a_low * b_low;

                        // High nibble (bits 4-7)
                        float a_high = decode_fp4_e2m1((a_packed >> 4) & 0x0F) * scale_a;
                        float b_high = decode_fp4_e2m1((b_packed >> 4) & 0x0F) * scale_b;
                        local_sum += a_high * b_high;
                    }
                }
            }

            acc[r] += local_sum;
        }

        __syncthreads();
    }

    // Write outputs
    #pragma unroll
    for (int r = 0; r < kRowsPerThread; r++) {
        const int local_row = tid * kRowsPerThread + r;
        if (local_row >= kTileM) continue;

        const int m_idx = m_start + local_row;
        if (m_idx < M) {
            D[m_idx] = __float2half(acc[r]);
        }
    }
}

// Host function to launch FP4 GEMV with dynamic shared memory
void launch_fp4_gemv_dynamic(
    torch::Tensor A, torch::Tensor B,
    torch::Tensor SFA, torch::Tensor SFB,
    torch::Tensor D,
    int64_t M, int64_t K, int64_t L
) {
    // Get raw pointers (no caching of state between iterations)
    const uint8_t* A_ptr = A.data_ptr<uint8_t>();
    const uint8_t* B_ptr = B.data_ptr<uint8_t>();
    const uint8_t* SFA_ptr = SFA.data_ptr<uint8_t>();
    const uint8_t* SFB_ptr = SFB.data_ptr<uint8_t>();
    half* D_ptr = reinterpret_cast<half*>(D.data_ptr<at::Half>());

    // Dimensions
    const int64_t K_packed = K / 2;      // FP4 is packed 2 per byte
    const int64_t K_scales = K / 16;     // Scale factors every 16 elements

    // Calculate dynamic shared memory size
    // A_smem: kTileM * kTileK_packed
    // B_smem: kTileK_packed
    // SFA_smem: kTileM * (kTileK/16)
    // SFB_smem: (kTileK/16)
    const size_t smem_size = kTileM * kTileK_packed + kTileK_packed +
                             kTileM * (kTileK / 16) + (kTileK / 16);

    // B200/SM100 requires explicit opt-in for dynamic shared memory > 48KB
    // Max is 227 KB per block on B200
    // Call only once to avoid racing conditions
    static bool smem_configured = false;
    if (!smem_configured) {
        cudaError_t err = cudaFuncSetAttribute(
            fp4_gemv_dynamic_smem_kernel,
            cudaFuncAttributeMaxDynamicSharedMemorySize,
            smem_size
        );
        if (err != cudaSuccess) {
            throw std::runtime_error(
                std::string("Failed to set dynamic shared memory size: ") +
                cudaGetErrorString(err) +
                " (requested " + std::to_string(smem_size) + " bytes)"
            );
        }
        smem_configured = true;
    }

    // Grid configuration
    const int num_blocks = (M + kTileM - 1) / kTileM;
    dim3 grid(num_blocks);
    dim3 block(kBlockSize);

    // Run batched GEMV (no state caching between batches)
    for (int64_t batch = 0; batch < L; batch++) {
        // Batch offsets - B tensor is padded to 128 rows
        const uint8_t* A_batch = A_ptr + batch * M * K_packed;
        const uint8_t* B_batch = B_ptr + batch * 128 * K_packed;  // B padded to 128 rows
        const uint8_t* SFA_batch = SFA_ptr + batch * M * K_scales;
        const uint8_t* SFB_batch = SFB_ptr + batch * 128 * K_scales;  // B scale factors, 128 rows
        half* D_batch = D_ptr + batch * M;

        // Launch kernel with dynamic shared memory
        fp4_gemv_dynamic_smem_kernel<<<grid, block, smem_size>>>(
            A_batch, B_batch, SFA_batch, SFB_batch, D_batch, M, K
        );
    }

    // Check for errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error(std::string("CUDA kernel launch error: ") + cudaGetErrorString(err));
    }

    // Synchronize to catch runtime errors
    err = cudaDeviceSynchronize();
    if (err != cudaSuccess) {
        throw std::runtime_error(std::string("CUDA sync error: ") + cudaGetErrorString(err));
    }
}
"""

cpp_source = """
void launch_fp4_gemv_dynamic(
    torch::Tensor A, torch::Tensor B,
    torch::Tensor SFA, torch::Tensor SFB,
    torch::Tensor D,
    int64_t M, int64_t K, int64_t L
);
"""

module = None


def get_module():
    global module
    if module is None:
        module = load_inline(
            name="nvfp4_gemv_dynamic_smem",
            cpp_sources=cpp_source,
            cuda_sources=cuda_source,
            functions=["launch_fp4_gemv_dynamic"],
            verbose=True,
            extra_cuda_cflags=[
                "-O3",
                "--use_fast_math",
                "-std=c++17",
                "-arch=sm_100a",
                "--expt-relaxed-constexpr",
                "-Xcudafe", "--diag_suppress=20012",
                f"-I{cutlass_path}/include",
                f"-I{cutlass_path}/tools/util/include",
            ],
            extra_ldflags=["-lcuda"],
        )
    return module


def custom_kernel(data: input_t) -> output_t:
    """
    B200-optimized FP4 GEMV using dynamic shared memory with large tiles.

    Uses extern __shared__ for 256x512 tiles, achieving speed-of-light performance.
    Target: < 10 Âµs geom_mean
    """
    a, b, sfa_ref_cpu, sfb_ref_cpu, _, _, c = data

    M, _, L = c.shape
    K = a.shape[1] * 2

    # Permute to [L, M, K/2] layout for matrix data
    a = a.permute(2, 0, 1).cuda().contiguous()
    b = b.permute(2, 0, 1).cuda().contiguous()
    c = c.permute(2, 0, 1).cuda().contiguous()

    # Reinterpret FP4 tensors as raw bytes (uint8)
    # FP4 tensors are stored as torch.float4_e2m1fn_x2 but underlying storage is uint8
    a_bytes = a.view(torch.uint8)
    b_bytes = b.view(torch.uint8)

    # Scale factors: convert from CPU to GPU and reinterpret as uint8
    # sfa_ref_cpu/sfb_ref_cpu are torch.float8_e4m3fn, need to view as uint8
    sfa = sfa_ref_cpu.permute(2, 0, 1).cuda().contiguous()
    sfb = sfb_ref_cpu.permute(2, 0, 1).cuda().contiguous()
    sfa_bytes = sfa.view(torch.uint8)
    sfb_bytes = sfb.view(torch.uint8)

    # Launch kernel with dynamic shared memory (no state caching)
    # Kernel expects: uint8 for A, B, SFA, SFB; float16 for D
    mod = get_module()
    mod.launch_fp4_gemv_dynamic(a_bytes, b_bytes, sfa_bytes, sfb_bytes, c, M, K, L)

    # Permute output back to [M, 1, L]
    c = c.permute(1, 2, 0).contiguous()

    return c
