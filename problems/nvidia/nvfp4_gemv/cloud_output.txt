Running on:
GPU: NVIDIA B200
CPU: INTEL(R) XEON(R) PLATINUM 8570
Runtime: CUDA
Platform: Linux-6.8.0-51-generic-x86_64-with-glibc2.35
Torch: 2.9.1+cu130


Benchmarks:
❌ k: 16384; l: 1; m: 7168; seed: 1111 failed testing:

mismatch found! custom implementation doesn't match reference: Number of mismatched elements: 7168 ERROR AT (0, 0, 0): 0.0 4668.0 ERROR AT (1, 0, 0): 0.0 4840.0 ERROR AT (2, 0, 0): 0.0 4756.0 ERROR AT (3, 0, 0): 0.0 4532.0 ERROR AT (4, 0, 0): 0.0 4812.0 ... and 7163 more mismatched elements.

❌ k: 7168; l: 8; m: 4096; seed: 1111 failed testing:

mismatch found! custom implementation doesn't match reference: Number of mismatched elements: 32768 ERROR AT (0, 0, 0): 0.0 2244.0 ERROR AT (0, 0, 1): 0.0 2168.0 ERROR AT (0, 0, 2): 0.0 1782.0 ERROR AT (0, 0, 3): 0.0 1916.0 ERROR AT (0, 0, 4): 0.0 1786.0 ... and 32763 more mismatched elements.

❌ k: 2048; l: 4; m: 7168; seed: 1111 failed testing:

mismatch found! custom implementation doesn't match reference: Number of mismatched elements: 28672 ERROR AT (0, 0, 0): 0.0 504.75 ERROR AT (0, 0, 1): 0.0 642.0 ERROR AT (0, 0, 2): 0.0 513.0 ERROR AT (0, 0, 3): 0.0 555.5 ERROR AT (1, 0, 0): 0.0 660.5 ... and 28667 more mismatched elements.
Cluster Bot
APP
 — 10:20 AM
Program stdout (1/9):
[SCALE DEBUG] sfa_ref_cpu shape=torch.Size([7168, 1024, 1]), device=cuda:0
[SCALE DEBUG] sfb_ref_cpu shape=torch.Size([128, 1024, 1]), device=cuda:0
[DEBUG] a_bytes: shape=(1, 7168, 8192), stride=(58720256, 8192, 1), elem_size=1, numel=58720256, bytes=58720256, data_ptr=0x7ffc66000000
[DEBUG] b_bytes: shape=(1, 128, 8192), stride=(1048576, 8192, 1), elem_size=1, numel=1048576, bytes=1048576, data_ptr=0x7ffca3800000
[DEBUG] sfa_bytes: shape=(1, 7168, 1024), stride=(7340032, 1024, 1), elem_size=1, numel=7340032, bytes=7340032, data_ptr=0x7ffc6c000000
[DEBUG] sfb_bytes: shape=(1, 128, 1024), stride=(131072, 1024, 1), elem_size=1, numel=131072, bytes=131072, data_ptr=0x7ffc9fbce000
[DEBUG] a_bytes range: [0x7ffc66000000, 0x7ffc69800000) (58720256 bytes)
[DEBUG] b_bytes range: [0x7ffca3800000, 0x7ffca3900000) (1048576 bytes)
[DEBUG] sfa_bytes range: [0x7ffc6c000000, 0x7ffc6c700000) (7340032 bytes)
[DEBUG] sfb_bytes range: [0x7ffc9fbce000, 0x7ffc9fbee000) (131072 bytes)
[DEBUG] a shape=(1, 7168, 8192), stride=(58720256, 8192, 1), data_ptr=0x7ffc66000000
[DEBUG] b shape=(1, 128, 8192), stride=(1048576, 8192, 1), data_ptr=0x7ffca3800000
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ (Python) a_bytes 128-byte alignment check passed: 0x7ffc66000000
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ (Python) b_bytes 128-byte alignment check passed: 0x7ffca3800000
[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=nvfp4_gemv_sm100_ptx -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-13.0/include -isystem /usr/include/python3.10 -fPIC -std=c++17 -c /home/runner/.cache/torch_extensions/py310_cu130/nvfp4_gemv_sm100_ptx/main.cpp -o main.o
Program stdout (2/9):
[2/3] /usr/local/cuda-13.0/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=nvfp4_gemv_sm100_ptx -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-13.0/include -isystem /usr/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -gencode=arch=compute_100a,code=sm_100a --expt-relaxed-constexpr --expt-extended-lambda -Xcudafe --diag_suppress=20012 -maxrregcount=128 --ptxas-options=-v,-warn-lmem-usage -lineinfo -I/usr/local/cutlass/include -c /home/runner/.cache/torch_extensions/py310_cu130/nvfp4_gemv_sm100_ptx/cuda.cu -o cuda.cuda.o
/home/runner/.cache/torch_extensions/py310_cu130/nvfp4_gemv_sm100_ptx/cuda.cu(1162): warning #177-D: variable "K_scales" was declared but never referenced
      const uint64_t K_scales = static_cast<uint64_t>(K / 16);
                     ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/home/runner/.cache/torch_extensions/py310_cu130/nvfp4_gemv_sm100_ptx/cuda.cu(1164): warning #177-D: variable "use_tma_a" was declared but never referenced
      bool use_tma_a = true;
           ^

/home/runner/.cache/torch_extensions/py310_cu130/nvfp4_gemv_sm100_ptx/cuda.cu(888): warning #177-D: variable "nmblocks_sfa" was declared but never referenced
              int nmblocks_sfa = (M + 127) / 128;
                  ^

/home/runner/.cache/torch_extensions/py310_cu130/nvfp4_gemv_sm100_ptx/cuda.cu(889): warning #177-D: variable "nkblocks_sfa" was declared but never referenced
              int nkblocks_sfa = (K_scales + 3) / 4;
                  ^
Program stdout (3/9):
ptxas warning : Local memory used for function '_Z18fp4_gemv_streamingILi128ELi128ELi320EEvPKhS1_S1_S1_PK14CUtensorMap_stS4_S4_P6__halfiii', size of stack frame: 192 bytes
ptxas info    : 7720 bytes gmem
ptxas info    : Compiling entry function '_Z18fp4_gemv_streamingILi128ELi128ELi320EEvPKhS1_S1_S1_PK14CUtensorMap_stS4_S4_P6__halfiii' for 'sm_100a'
ptxas info    : Function properties for _Z18fp4_gemv_streamingILi128ELi128ELi320EEvPKhS1_S1_S1_PK14CUtensorMap_stS4_S4_P6__halfiii
    192 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 90 registers, used 1 barriers, 192 bytes cumulative stack size, 16 bytes smem
ptxas info    : Compile time = 348.933 ms
[3/3] c++ main.o cuda.cuda.o -shared -lcuda -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-13.0/lib64 -lcudart -o nvfp4_gemv_sm100_ptx.so
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ A_ptr 128-byte alignment check passed: 0x7ffc66000000
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ K_packed 128-byte stride alignment check passed: 8192 bytes
TMA Debug: A_ptr = 0x7ffc66000000, map_A_ptr = 0x5555622c3400
WARNING: box_k (64) is not a multiple of 128 elements, may reduce TMA efficiency
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ Tile size 128-byte alignment check passed: 8192 bytes
TMA Debug: Using RANK=2 for L=1
TMA Debug: dims = [K_packed=8192, M=7168], box = [64, 128]
TMA Debug: strides_A[0] = 8192 bytes
TMA Encode A Result: 0
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ TMA Encode A (rank=2) SUCCESS!
DEBUG launch grid=(56,1) blockDim.x=320 shared_bytes=108672 M=7168 K=16384 L=1
=== KERNEL DEBUG (block 0,0) ===
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=96
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=97
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=98
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=99
Program stdout (4/9):
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=100
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=101
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=102
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=103
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=104
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=105
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=106
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=107
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=108
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=109
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=110
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=111
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=112
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=113
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=114
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=115
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=116
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=117
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=118
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=119
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=120
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=121
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=122
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=123
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=124
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=125
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=126
IN_LOOP B/SFB cp_async START block=(17,0,0) tid=127
DBG block=(51,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=6528 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(50,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=6400 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(55,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=7040 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
Program stdout (5/9):
DBG block=(54,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=6912 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(44,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=5632 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(45,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=5760 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(26,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=3328 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(27,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=3456 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(28,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=3584 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(29,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=3712 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(49,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=6272 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(31,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=3968 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(32,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=4096 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(30,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=3840 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(33,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=4224 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(34,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=4352 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(35,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=4480 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(37,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=4736 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
Program stdout (6/9):
DBG block=(36,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=4608 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(14,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=1792 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(15,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=1920 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(7,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=896 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(8,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=1024 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(9,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=1152 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(48,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=6144 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(43,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=5504 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(42,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=5376 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(21,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=2688 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(20,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=2560 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(22,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=2816 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(24,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=3072 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
DBG block=(25,0,0) tid=0 warp_id=0 lane_id=0 batch=0 m_tile=3200 tile_rows=128 M=7168 K=16384 L=1 K_packed=8192 K_scales=1024
SFB raw bytes [0-3]: 0x40 0x38 0x38 0x00
SFB decoded FP8 [0-3]: 2.000000 1.000000 1.000000 0.000000
Program stdout (7/9):
B packed bytes [0-3]: 0x03 0x01 0x01 0x00
SFB contiguous_idx[row=0,col=0,batch=0] = 0
SFB contiguous_idx[row=0,col=1,batch=0] = 1
SFB contiguous_idx[row=0,col=2,batch=0] = 2
SFB contiguous_idx[row=0,col=3,batch=0] = 3
b_vec_smem decoded [0-7]: 0.000000 3.000000 0.000000 1.000000 0.000000 1.000000 0.000000 0.000000
SFA raw bytes [row=0, 0-3]: 0x40 0x00 0x40 0x40
SFA decoded FP8 [row=0, 0-3]: 2.000000 0.000000 2.000000 2.000000
A packed bytes [row=0, 0-3]: 0x03 0x00 0x00 0x03
A decoded [row=0, 0-7]: 0.000000 3.000000 0.000000 0.000000 0.000000 0.000000 0.000000 3.000000
SFA contiguous_idx[row=0,col=0,batch=0] = 0
SFA contiguous_idx[row=0,col=1,batch=0] = 1
SFA contiguous_idx[row=0,col=2,batch=0] = 2
SFA contiguous_idx[row=0,col=3,batch=0] = 3
=== END KERNEL DEBUG ===
MMA DEBUG: k_tile=0 kk=0 warp=2 lane=0 c_frag=[0.0000, 30.0000, 0.0000, 28.5000]
FINAL ACCUM: warp=2 lane=0 c_frag_0=0.0000 c_frag_2=0.0000 (rows 0,8)
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ A_ptr 128-byte alignment check passed: 0x7ffc6c000000
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ K_packed 128-byte stride alignment check passed: 8192 bytes
TMA Debug: A_ptr = 0x7ffc6c000000, map_A_ptr = 0x55556a678180
WARNING: box_k (64) is not a multiple of 128 elements, may reduce TMA efficiency
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ Tile size 128-byte alignment check passed: 8192 bytes
TMA Debug: Using RANK=2 for L=1
TMA Debug: dims = [K_packed=8192, M=7168], box = [64, 128]
TMA Debug: strides_A[0] = 8192 bytes
TMA Encode A Result: 0
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ TMA Encode A (rank=2) SUCCESS!
DEBUG launch grid=(56,1) blockDim.x=320 shared_bytes=108672 M=7168 K=16384 L=1
=== KERNEL DEBUG (block 0,0) ===
SFB raw bytes [0-3]: 0x40 0x38 0x38 0x00
SFB decoded FP8 [0-3]: 2.000000 1.000000 1.000000 0.000000
B packed bytes [0-3]: 0x03 0x01 0x01 0x00
SFB contiguous_idx[row=0,col=0,batch=0] = 0
Program stdout (8/9):
SFB contiguous_idx[row=0,col=1,batch=0] = 1
SFB contiguous_idx[row=0,col=2,batch=0] = 2
SFB contiguous_idx[row=0,col=3,batch=0] = 3
b_vec_smem decoded [0-7]: 0.000000 3.000000 0.000000 1.000000 0.000000 1.000000 0.000000 0.000000
SFA raw bytes [row=0, 0-3]: 0x40 0x00 0x40 0x40
SFA decoded FP8 [row=0, 0-3]: 2.000000 0.000000 2.000000 2.000000
A packed bytes [row=0, 0-3]: 0x03 0x00 0x00 0x03
A decoded [row=0, 0-7]: 0.000000 3.000000 0.000000 0.000000 0.000000 0.000000 0.000000 3.000000
SFA contiguous_idx[row=0,col=0,batch=0] = 0
SFA contiguous_idx[row=0,col=1,batch=0] = 1
SFA contiguous_idx[row=0,col=2,batch=0] = 2
SFA contiguous_idx[row=0,col=3,batch=0] = 3
=== END KERNEL DEBUG ===
MMA DEBUG: k_tile=0 kk=0 warp=2 lane=0 c_frag=[0.0000, 30.0000, 0.0000, 28.5000]
FINAL ACCUM: warp=2 lane=0 c_frag_0=0.0000 c_frag_2=0.0000 (rows 0,8)
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ A_ptr 128-byte alignment check passed: 0x7ffc08000000
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ K_packed 128-byte stride alignment check passed: 3584 bytes
TMA Debug: A_ptr = 0x7ffc08000000, map_A_ptr = 0x55556a678180
WARNING: box_k (64) is not a multiple of 128 elements, may reduce TMA efficiency
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ Tile size 128-byte alignment check passed: 8192 bytes
TMA Debug: Using RANK=3 for L=8
TMA Debug: dims = [K_packed=3584, M=4096, L=8], box = [64, 128, 1]
TMA Debug: strides_A = [3584, 14680064]
TMA Encode A Result: 0
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ TMA Encode A (rank=3) SUCCESS!
DEBUG launch grid=(32,8) blockDim.x=320 shared_bytes=85120 M=4096 K=7168 L=8
=== KERNEL DEBUG (block 0,0) ===
SFB raw bytes [0-3]: 0x38 0x00 0x40 0x38
SFB decoded FP8 [0-3]: 1.000000 0.000000 2.000000 1.000000
B packed bytes [0-3]: 0x03 0x03 0x01 0x02
SFB contiguous_idx[row=0,col=0,batch=0] = 0
SFB contiguous_idx[row=0,col=1,batch=0] = 1
SFB contiguous_idx[row=0,col=2,batch=0] = 2
Program stdout (9/9):
SFB contiguous_idx[row=0,col=3,batch=0] = 3
b_vec_smem decoded [0-7]: 0.000000 1.500000 0.000000 1.500000 0.000000 0.500000 0.000000 1.000000
SFA raw bytes [row=0, 0-3]: 0x40 0x00 0x00 0x38
SFA decoded FP8 [row=0, 0-3]: 2.000000 0.000000 0.000000 1.000000
A packed bytes [row=0, 0-3]: 0x03 0x00 0x00 0x03
A decoded [row=0, 0-7]: 0.000000 3.000000 0.000000 0.000000 0.000000 0.000000 0.000000 3.000000
SFA contiguous_idx[row=0,col=0,batch=0] = 0
SFA contiguous_idx[row=0,col=1,batch=0] = 1
SFA contiguous_idx[row=0,col=2,batch=0] = 2
SFA contiguous_idx[row=0,col=3,batch=0] = 3
=== END KERNEL DEBUG ===
MMA DEBUG: k_tile=0 kk=0 warp=2 lane=0 c_frag=[0.0000, 16.5000, 0.0000, 18.0000]
FINAL ACCUM: warp=2 lane=0 c_frag_0=0.0000 c_frag_2=0.0000 (rows 0,8)
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ A_ptr 128-byte alignment check passed: 0x7ffc5a000000
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ K_packed 128-byte stride alignment check passed: 1024 bytes
TMA Debug: A_ptr = 0x7ffc5a000000, map_A_ptr = 0x55556a678180
WARNING: box_k (64) is not a multiple of 128 elements, may reduce TMA efficiency
ÃƒÆ’Ã‚Â¢Ãƒâ€¦Ã¢â‚¬Å“ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œ Tile size 128-byte alignment check passed: 8192 bytes
TMA Debug: Using RANK=3 for L=4
TMA Debug: dims = [K_packed=1024, M=7168, L=4], box = [64, 128, 1]
TMA Debug: strides_A = [1024, 7340032]
TMA Encode A Result: 0
[...] 68 lines omitted
