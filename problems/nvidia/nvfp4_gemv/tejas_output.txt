Running on:
GPU: NVIDIA B200
CPU: INTEL(R) XEON(R) PLATINUM 8570
Runtime: CUDA
Platform: Linux-6.8.0-51-generic-x86_64-with-glibc2.35
Torch: 2.9.1+cu130
Running failed
Command
python3 eval.py benchmark /tmp/tmppc447s6q

exited with error code 1 after 10.91 seconds.
Cluster Bot
APP
 — 12:35 PM
Program stderr (1/2):
multiprocessing.pool.RemoteTraceback:
"""
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py", line 2597, in _run_ninja_build
    subprocess.run(
  File "/usr/lib/python3.10/subprocess.py", line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 2.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py", line 217, in _run_single_benchmark
    output = custom_kernel(_clone_data(data))
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/submission.py", line 1492, in custom_kernel
    mod = get_module()
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/submission.py", line 1342, in get_module
    module = load_inline(
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py", line 2051, in load_inline
    return _jit_compile(
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py", line 2134, in _jit_compile
    _write_ninja_file_and_build_library(
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py", line 2286, in _write_ninja_file_and_build_library
    _run_ninja_build(
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py", line 2614, in _run_ninja_build
    raise RuntimeError(message) from e
RuntimeError: Error building extension 'nvfp4_gemv_sm100_ptx'
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
Program stderr (2/2):
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py", line 485, in <module>
    sys.exit(main())
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py", line 453, in main
    return run_benchmarking(logger, pool, tests)
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py", line 306, in run_benchmarking
    run_single_benchmark(pool, tests[0], False, 200, 10e7)
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py", line 291, in run_single_benchmark
    return pool.apply(_run_single_benchmark, (test, recheck, max_repeats, max_time_ns))
  File "/usr/lib/python3.10/multiprocessing/pool.py", line 360, in apply
    return self.apply_async(func, args, kwds).get()
  File "/usr/lib/python3.10/multiprocessing/pool.py", line 774, in get
    raise self._value
RuntimeError: Error building extension 'nvfp4_gemv_sm100_ptx'
Program stdout (1/3):
[SCALE DEBUG] sfa_ref_cpu shape=torch.Size([7168, 1024, 1]), device=cuda:0
[SCALE DEBUG] sfb_ref_cpu shape=torch.Size([128, 1024, 1]), device=cuda:0
[DEBUG] a_bytes: shape=(1, 7168, 8192), stride=(58720256, 8192, 1), elem_size=1, numel=58720256, bytes=58720256, data_ptr=0x7ffc1e000000
[DEBUG] b_bytes: shape=(1, 128, 8192), stride=(1048576, 8192, 1), elem_size=1, numel=1048576, bytes=1048576, data_ptr=0x7ffc5b800000
[DEBUG] sfa_bytes: shape=(1, 7168, 1024), stride=(7340032, 1024, 1), elem_size=1, numel=7340032, bytes=7340032, data_ptr=0x7ffc24000000
[DEBUG] sfb_bytes: shape=(1, 128, 1024), stride=(131072, 1024, 1), elem_size=1, numel=131072, bytes=131072, data_ptr=0x7ffc57bce000
[DEBUG] a_bytes range: [0x7ffc1e000000, 0x7ffc21800000) (58720256 bytes)
[DEBUG] b_bytes range: [0x7ffc5b800000, 0x7ffc5b900000) (1048576 bytes)
[DEBUG] sfa_bytes range: [0x7ffc24000000, 0x7ffc24700000) (7340032 bytes)
[DEBUG] sfb_bytes range: [0x7ffc57bce000, 0x7ffc57bee000) (131072 bytes)
[DEBUG] a shape=(1, 7168, 8192), stride=(58720256, 8192, 1), data_ptr=0x7ffc1e000000
[DEBUG] b shape=(1, 128, 8192), stride=(1048576, 8192, 1), data_ptr=0x7ffc5b800000
âœ…(Python) a_bytes 128-byte alignment check passed: 0x7ffc1e000000
âœ…(Python) b_bytes 128-byte alignment check passed: 0x7ffc5b800000
Program stdout (2/3):
[1/2] /usr/local/cuda-13.0/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=nvfp4_gemv_sm100_ptx -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-13.0/include -isystem /usr/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -gencode=arch=compute_100a,code=sm_100a --expt-relaxed-constexpr --expt-extended-lambda -Xcudafe --diag_suppress=20012 -maxrregcount=128 --ptxas-options=-v,-warn-lmem-usage -lineinfo -I/usr/local/cutlass/include -c /home/runner/.cache/torch_extensions/py310_cu130/nvfp4_gemv_sm100_ptx/cuda.cu -o cuda.cuda.o
FAILED: [code=2] cuda.cuda.o
/usr/local/cuda-13.0/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=nvfp4_gemv_sm100_ptx -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-13.0/include -isystem /usr/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -gencode=arch=compute_100a,code=sm_100a --expt-relaxed-constexpr --expt-extended-lambda -Xcudafe --diag_suppress=20012 -maxrregcount=128 --ptxas-options=-v,-warn-lmem-usage -lineinfo -I/usr/local/cutlass/include -c /home/runner/.cache/torch_extensions/py310_cu130/nvfp4_gemv_sm100_ptx/cuda.cu -o cuda.cuda.o
Program stdout (3/3):
/home/runner/.cache/torch_extensions/py310_cu130/nvfp4_gemv_sm100_ptx/cuda.cu(1306): error: no instance of function template "fp4_gemv_streaming" matches the argument list
            argument types are: (const uint8_t *, const uint8_t *, const uint8_t *, const uint8_t *, CUtensorMap *, CUtensorMap *, CUtensorMap *, half *, int, int, int)
      fp4_gemv_streaming<kTileM, kTileK, kThreads><<<grid, block, shared_bytes>>>(
      ^
/home/runner/.cache/torch_extensions/py310_cu130/nvfp4_gemv_sm100_ptx/cuda.cu(214): note #3322-D: number of parameters of function template "fp4_gemv_streaming" does not match the call
  fp4_gemv_streaming(
  ^

1 error detected in the compilation of "/home/runner/.cache/torch_extensions/py310_cu130/nvfp4_gemv_sm100_ptx/cuda.cu".
ninja: build stopped: subcommand failed.
