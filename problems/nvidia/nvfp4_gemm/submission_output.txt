RuntimeError: CUDA Error: MaxDynamicSharedMemorySize - invalid argument

Running on:
GPU: NVIDIA B200
CPU: INTEL(R) XEON(R) PLATINUM 8570
Runtime: CUDA
Platform: Linux-6.8.0-51-generic-x86_64-with-glibc2.35
Torch: 2.9.1+cu130
Running failed
Command
python3 eval.py benchmark /tmp/tmpz_we_7r2
exited with error code 1 after 33.27 seconds.
Cluster BotAPP â€” 10:05 PM
Program stderr (1/1):
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py", line 228, in _run_single_benchmark
    output = custom_kernel(_clone_data(data))
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/submission.py", line 1054, in custom_kernel
    mod.launch_fp4_gemm_optimized(
RuntimeError: CUDA Error: MaxDynamicSharedMemorySize - invalid argument
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py", line 498, in <module>
    sys.exit(main())
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py", line 466, in main
    return run_benchmarking(logger, pool, tests)
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py", line 319, in run_benchmarking
    run_single_benchmark(pool, tests[0], False, 200, 10e7)
  File "/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py", line 304, in run_single_benchmark
    return pool.apply(_run_single_benchmark, (test, recheck, max_repeats, max_time_ns))
  File "/usr/lib/python3.10/multiprocessing/pool.py", line 360, in apply
    return self.apply_async(func, args, kwds).get()
  File "/usr/lib/python3.10/multiprocessing/pool.py", line 774, in get
    raise self._value
RuntimeError: CUDA Error: MaxDynamicSharedMemorySize - invalid argument
Program stdout:
A shape: torch.Size([128, 8192, 1]), stride: (8192, 1, 1048576), ptr: 0x7ff7f3500000
B shape: torch.Size([7168, 8192, 1]), stride: (8192, 1, 58720256), ptr: 0x7ff7e2000000
A aligned: True
B aligned: True
TMA params: M=128, N=7168, K=16384, K_packed=8192
Box dims: [128, 128]
Global dims A: [8192, 128]
Global dims B: [8192, 7168]

=== Python Tensor Debug ===
SFA device: cuda:0, dtype: torch.float8_e4m3fn, shape: torch.Size([128, 1024])
SFB device: cuda:0, dtype: torch.float8_e4m3fn, shape: torch.Size([7168, 1024])
SFA is_contiguous: True, data_ptr: 0x7ff7e5800000
SFB is_contiguous: True, data_ptr: 0x7ff7de000000
SFA_bytes device: cuda:0, shape: torch.Size([128, 1024]), stride: (1024, 1)
SFB_bytes device: cuda:0, shape: torch.Size([7168, 1024]), stride: (1024, 1)
K_scales: 1024, K_scales_padded: 1024
Expected SFA size: M=128 x K_scales_padded=1024
Expected SFB size: N=7168 x K_scales_padded=1024
===========================

TMA A: dims=[8192, 128], box=[128, 128], strides=[8192], ptr=0x7ff7f3500000
TMA B: dims=[8192, 7168], box=[128, 128], strides=[8192], ptr=0x7ff7e2000000
TMA SFA: dims=[1024, 128], box=[128, 128], strides=[1024], ptr=0x7ff7e5800000, K_scales_padded=1024
TMA SFB: dims=[1024, 7168], box=[128, 128], strides=[1024], ptr=0x7ff7de000000, K_scales_padded=1024