git --no-pager diff submission.py
diff --git a/problems/nvidia/nvfp4_gemm/submission.py b/problems/nvidia/nvfp4_gemm/submission.py
index 890c192..8946a43 100644
--- a/problems/nvidia/nvfp4_gemm/submission.py
+++ b/problems/nvidia/nvfp4_gemm/submission.py
@@ -3,7 +3,7 @@ import os
 import torch
 from task import input_t, output_t
 from torch.utils.cpp_extension import load_inline
-from reference import to_blocked
+# No longer need to_blocked - using sfa_permuted (atom-tiled) directly
 
 cutlass_path = os.environ.get("CUTLASS_PATH", "/usr/local/cutlass")
 
@@ -519,7 +519,7 @@ template<int TileM, int TileK>
 __device__ __forceinline__ void prefetch_tile(
     int stage, int k_tile_base,
     bool use_tma_a, bool is_producer, int warp_id, int lane_id,
-    int m_tile, int n_tile, int K_packed, int K_scales_padded, int M, int N,
+    int m_tile, int n_tile, int K_packed, int K_scales, int M, int N,
     uint8_t** a_packed_stage, uint8_t** b_packed_stage, uint8_t** sfa_stage, uint8_t** sfb_stage,
     uint64_t* mbar_a, uint64_t* mbar_b,
     const CUtensorMap* desc_A, const CUtensorMap* desc_B,
@@ -551,16 +551,17 @@ __device__ __forceinline__ void prefetch_tile(
             // bool valid_k = (c_k_packed + TileKPacked) <= static_cast<uint32_t>(K_packed);
             // bool valid_m = (c_m + TileM) <= static_cast<uint32_t>(M);
 
-            // --- TMA Load SFA (to_blocked layout: 4 tiles of 32×16=512 bytes per K-tile) ---
-            // to_blocked produces (4, 32, 16) for each M-tile × K-tile
+            // --- TMA Load SFA (atom-tiled layout: 2048 bytes per K-tile) ---
+            // Atom-tiled: (32, 4, rest_m, 4, rest_k) flattened → 2048 bytes
             // TMA box is [16, 32] = 512 bytes, need 4 loads to get 2048 bytes total
             int n_row_blocks = (M + 127) / 128;
-            int n_col_blocks = (K_scales_padded + 3) / 4;
+            int n_col_blocks = (K_scales + 3) / 4;
             int m_block_sfa = c_m / TileM;
             int k_tile_idx_sfa = k_tile_base / TileK;
             
-            // Base tile index in to_blocked output: (m_block * n_col_blocks + k_tile_idx) * 4
-            int base_tile_sfa = (m_block_sfa * n_col_blocks + k_tile_idx_sfa) * 4;
+            // Base tile index: m_block * n_col_blocks + k_tile_idx * 4
+            // k_tile_idx * 4 because each kernel K-tile spans 4 column blocks in atom-tiled layout
+            int base_tile_sfa = m_block_sfa * n_col_blocks + k_tile_idx_sfa * 4;
             bool valid_sfa = valid_m;
 
             // Calculate expected bytes for mbar_a (4 × 512 = 2048 bytes for scales)
@@ -594,12 +595,13 @@ __device__ __forceinline__ void prefetch_tile(
             // Relaxed guard for N
             bool valid_n = (c_n < N);
 
-            // --- TMA Load SFB (to_blocked layout: 4 tiles of 32×16=512 bytes per K-tile) ---
+            // --- TMA Load SFB (atom-tiled layout: 2048 bytes per K-tile) ---
             int n_row_blocks_sfb = (N + 127) / 128;
-            int n_col_blocks_sfb = (K_scales_padded + 3) / 4;
+            int n_col_blocks_sfb = (K_scales + 3) / 4;
             int n_block_sfb = c_n / TileM;  // TileN = TileM = 128
             int k_tile_idx_sfb = k_tile_base / TileK;
-            int base_tile_sfb = (n_block_sfb * n_col_blocks_sfb + k_tile_idx_sfb) * 4;
+            // Base tile index: n_block * n_col_blocks + k_tile_idx * 4
+            int base_tile_sfb = n_block_sfb * n_col_blocks_sfb + k_tile_idx_sfb * 4;
             bool valid_sfb = valid_n;
 
             // Calculate expected bytes for mbar_b (4 × 512 = 2048 bytes for scales)
@@ -644,7 +646,7 @@ __device__ __forceinline__ void process_tile(
     const uint8_t* __restrict__ SFA_packed,
     const uint8_t* __restrict__ SFB_packed,
     half* a_f16_smem, half* b_f16_smem,
-    const int M, const int N, const int K, const int K_scales_padded,
+    const int M, const int N, const int K, const int K_scales,
     const int tid, const int warp_id, const int lane_id,
     const bool is_producer, const bool is_consumer,
     float c_accum[16][4]
@@ -660,7 +662,7 @@ __device__ __forceinline__ void process_tile(
     constexpr int SfaBoxK = 128;
 
     const int K_packed = K >> 1;
-    const int K_scales = (K + 15) >> 4; // one FP8 scale per 16 FP4 values
+    // K_scales is already passed as parameter
 
     // Base K indices for this tile
     const int k_packed_base = k_tile >> 1;
@@ -900,7 +902,7 @@ fp4_gemm_rank2_cta(
     const CUtensorMap* __restrict__ desc_B,
     const CUtensorMap* __restrict__ desc_SFB,
     half* __restrict__ D,
-    const int M, const int N, const int K, const int L, const int K_scales_padded
+    const int M, const int N, const int K, const int L, const int K_scales
 ) {
 #if __CUDA_ARCH__ >= 900
     constexpr int TileKPacked = TileK / 2;
@@ -1044,7 +1046,7 @@ fp4_gemm_rank2_cta(
         if (k_tile < K) {
             prefetch_tile<TileM, TileK>(
                 s, k_tile, use_tma_a, is_producer, warp_id, lane_id,
-                m_tile, n_tile, K_packed, K_scales_padded, M, N,
+                m_tile, n_tile, K_packed, K_scales, M, N,
                 a_packed_stage, b_packed_stage, sfa_stage, sfb_stage,
                 mbar_a, mbar_b,
                 desc_A, desc_B, desc_SFA, desc_SFB
@@ -1060,7 +1062,7 @@ fp4_gemm_rank2_cta(
         if (next_k < K) {
             prefetch_tile<TileM, TileK>(
                 (stage + StageCount - 1) % StageCount, next_k, use_tma_a, is_producer, warp_id, lane_id,
-                m_tile, n_tile, K_packed, K_scales_padded, M, N,
+                m_tile, n_tile, K_packed, K_scales, M, N,
                 a_packed_stage, b_packed_stage, sfa_stage, sfb_stage,
                 mbar_a, mbar_b,
                 desc_A, desc_B, desc_SFA, desc_SFB
@@ -1079,7 +1081,7 @@ fp4_gemm_rank2_cta(
             a_packed_stage, b_packed_stage, sfa_stage, sfb_stage,
             A_packed, B_packed, SFA_packed, SFB_packed,
             a_f16_smem, b_f16_smem,
-            M, N, K, K_scales_padded,
+            M, N, K, K_scales,
             tid, warp_id, lane_id,
             is_producer, is_consumer,
             c_accum
@@ -1160,7 +1162,7 @@ fp4_gemm_rank2_cta(
         if (k_tile < K) {
             prefetch_tile<TileM, TileK>(
                 s, k_tile, use_tma_a, is_producer, warp_id, lane_id,
-                m_tile, n_tile, K_packed, K_scales_padded, M, N,
+                m_tile, n_tile, K_packed, K_scales, M, N,
                 a_packed_stage, b_packed_stage, sfa_stage, sfb_stage,
                 mbar_a, mbar_b,
                 desc_A, desc_B, desc_SFA, desc_SFB
@@ -1191,24 +1193,21 @@ fp4_gemm_rank2_cta(
     uint32_t tmem_c = tmem_base_ptr_tcgen05;
     
     // TMEM addresses for scale factors
-    // TMEM addressing format: bits [15:0] = column index, bits [31:16] = DP (data path)
-    // For block_scale.block16, each K-tile needs TileM × (TileK/16) = 128 × 16 = 2048 scale bytes
+    // CRITICAL: TMEM addresses are BYTE-ADDRESSED, not column indices!
+    // Format: bits [15:0] = byte offset, bits [31:16] = DP (data path/row)
     //
-    // TMEM Layout (256 columns allocated):
-    //   - Columns 0-127: Accumulator (128×128 floats, uses all 128 DPs × 128 columns)
-    //   - Columns 128-143: SFA scale factors (16 columns for TileK/16 = 16 scales per row)
-    //   - Columns 144-159: SFB scale factors (16 columns)
+    // Memory layout:
+    //   - Bytes 0-511: Accumulator C (128 columns × 4 bytes/col = 512 bytes)
+    //   - Bytes 512-575: SFA scale factors (16 columns × 4 bytes = 64 bytes)
+    //   - Bytes 576-639: SFB scale factors (16 columns × 4 bytes = 64 bytes)
     //
-    // Column offset (not byte offset!) is added to tmem_c
-    constexpr int ACCUM_COLS = 128;      // Accumulator uses 128 columns
-    constexpr int SF_COLS = 16;          // Each scale tensor uses 16 columns (TileK/16)
-    
-    constexpr int TMEM_COLS = 256;
-    constexpr int TMEM_ROW_PITCH = TMEM_COLS * sizeof(float);  // 1024 bytes per TMEM row for epilogue
+    // TMEM base addresses (byte offsets from tmem_c)
+    constexpr int ACCUM_BYTE_SIZE = 128 * sizeof(float);  // 512 bytes
+    constexpr int SF_BYTE_SIZE = 16 * sizeof(float);       // 64 bytes
     
-    // Scale factor TMEM addresses (column offsets)
-    uint32_t tmem_sfa_base = tmem_c + ACCUM_COLS;           // SFA at column 128
-    uint32_t tmem_sfb_base = tmem_c + ACCUM_COLS + SF_COLS; // SFB at column 144
+    // Scale factor TMEM addresses (BYTE offsets, not column offsets!)
+    uint32_t tmem_sfa_base = tmem_c + ACCUM_BYTE_SIZE;           // tmem_c + 512
+    uint32_t tmem_sfb_base = tmem_c + ACCUM_BYTE_SIZE + SF_BYTE_SIZE;  // tmem_c + 576
 
     // =========================================================================
     // PERMUTED SCALE BUFFERS FOR TCGEN05
@@ -1216,7 +1215,8 @@ fp4_gemm_rank2_cta(
     // TMA loads simple scales into sfa_stage/sfb_stage (128-byte row stride).
     // tcgen05.mma expects "blocked" layout scales in TMEM.
     // We allocate separate contiguous buffers for permuted scales, then
-    // call permute_scales_to_blocked to convert and tcgen05.cp to copy to TMEM.
+    // Scales are pre-loaded in atom-tiled layout via TMA.
+    // Use tcgen05.cp to copy from SMEM to TMEM.
     //
     // Size: TileM * (TileK/16) = 128 * 16 = 2048 bytes each
     constexpr int KScalesTile = TileK / 16;  // 16 for TileK=256
@@ -1226,43 +1226,8 @@ fp4_gemm_rank2_cta(
     __shared__ alignas(128) uint8_t sfa_permuted_smem[PERMUTED_SF_SIZE];
     __shared__ alignas(128) uint8_t sfb_permuted_smem[PERMUTED_SF_SIZE];
 
-    // =========================================================================
-    // CLEAR TMEM ACCUMULATOR
-    // =========================================================================
-    // tcgen05.mma accumulates (C += A*B). With scaleC=1, we must ensure C (TMEM)
-    // is zeroe-initialized before the first tile. tcgen05.alloc does NOT zero.
-    
-    // 1. Clear sfa_permuted_smem to use as a source of zeros
-    for (int i = tid; i < PERMUTED_SF_SIZE; i += Threads) {
-        sfa_permuted_smem[i] = 0;
-    }
-    __syncthreads();
-
-    // 2. Copy zeros to Accumulator columns (0-127) using tcgen05.cp
-    if (elect_one_sync_local()) {
-        // Create descriptor for the zero buffer
-        // Leading byte offset = 16 (matches what we use for scales, creates 2048-byte payload)
-        uint64_t desc_zero = make_sf_smem_desc(sfa_permuted_smem, 16, 0);
-
-        // Accumulator has 128 floats per row = 512 bytes per row.
-        // tcgen05.cp with our descriptor writes 16 bytes per row (based on scale usage).
-        // To fill 512 bytes, we need 512 / 16 = 32 copies per row-set.
-        // One tcgen05.cp covers 4 TMEM columns (if 1 col = 4 bytes).
-        // Total 128 accumulator columns. 128 / 4 = 32.
-        
-        #pragma unroll
-        for (int j = 0; j < 32; ++j) {
-            uint32_t tmem_dst = tmem_c + j * 4; // Stride by 4 columns
-            asm volatile(
-                "tcgen05.cp.cta_group::1.32x128b.warpx4 [%0], %1;\n"
-                :: "r"(tmem_dst), "l"(desc_zero)
-                : "memory"
-            );
-        }
-    }
-    __syncthreads(); // Ensure TMEM clear finishes (though cp is async-ish, wait handles it later? 
-                     // Actually cp needs to complete before mma? 
-                     // tcgen05 instructions issue in order. MMA will follow CP.)
+    // NOTE: TMEM accumulator initialization is handled by scaleC=0 on the first MMA.
+    // tcgen05.mma with scaleC=0 (predicate p=false) does D = A*B, ignoring existing C.
 
     #if __CUDA_ARCH__ >= 1000
     // Build instruction descriptor (constant for all K-tiles)
@@ -1285,7 +1250,7 @@ fp4_gemm_rank2_cta(
         if (next_k < K) {
             prefetch_tile<TileM, TileK>(
                 (stage + StageCount - 1) % StageCount, next_k, use_tma_a, is_producer, warp_id, lane_id,
-                m_tile, n_tile, K_packed, K_scales_padded, M, N,
+                m_tile, n_tile, K_packed, K_scales, M, N,
                 a_packed_stage, b_packed_stage, sfa_stage, sfb_stage,
                 mbar_a, mbar_b,
                 desc_A, desc_B, desc_SFA, desc_SFB
@@ -1299,30 +1264,21 @@ fp4_gemm_rank2_cta(
 
         #if __CUDA_ARCH__ >= 1000
         // =========================================================================
-        // Copy pre-permuted scales directly from SMEM to TMEM
+        // Copy scales from SMEM to TMEM for tcgen05.mma
         // =========================================================================
-        // TMA loaded pre-permuted scales directly into sfa_stage/sfb_stage.
-        // The data is already in blocked format (2048 bytes = 128 rows × 16 bytes).
-        // No in-kernel permutation needed - copy directly to TMEM.
         
         if (elect_one_sync_local()) {
-            // tcgen05.cp.cta_group::1.32x128b.warpx4 copies 4 TMEM columns per call (32 DPs × 128b)
-            // Scale factors need 16 columns each, so we need 4 iterations
-            // Each iteration:
-            //   - Reads 512 bytes from SMEM (32 rows × 16 bytes, with 16B row stride)
-            //   - Writes to 4 TMEM columns
-            //
-            // SMEM layout: 2048 bytes = 4 chunks × 512 bytes, each chunk = 32 rows × 16 bytes
+            // tcgen05.cp copies scale factors to TMEM
+            // tcgen05.mma reads scales from TMEM addresses
             
             #pragma unroll
             for (int j = 0; j < 4; ++j) {
-                // Each chunk is 512 bytes = 32 rows × 16 bytes
                 uint64_t desc_sfa = make_sf_smem_desc(sfa_stage[stage] + j * 512, 16, 0);
                 uint64_t desc_sfb = make_sf_smem_desc(sfb_stage[stage] + j * 512, 16, 0);
                 
-                // TMEM destination advances by 4 columns per chunk
-                uint32_t sfa_dst = tmem_sfa_base + j * 4;
-                uint32_t sfb_dst = tmem_sfb_base + j * 4;
+                // TMEM addressing: row in DP field (upper 16 bits)
+                uint32_t sfa_dst = tmem_sfa_base + (j * 32 << 16);
+                uint32_t sfb_dst = tmem_sfb_base + (j * 32 << 16);
                 
                 asm volatile(
                     "tcgen05.cp.cta_group::1.32x128b.warpx4 [%0], %1;\n"
@@ -1338,7 +1294,6 @@ fp4_gemm_rank2_cta(
         }
 
         __syncthreads();
-
         // Build SMEM descriptors for packed FP4 A and B
         // A: [TileM, TileK/2] packed FP4 in SMEM
         // B: [TileN, TileK/2] packed FP4 in SMEM
@@ -1355,13 +1310,13 @@ fp4_gemm_rank2_cta(
             0,
             0               // SWIZZLE_NONE
         );
-
+        
         // Issue tcgen05.mma.kind::mxf4nvf4.block_scale.block16
-        // scaleC controls predicate p: setp.ne.b32 p, scaleC, 0
-        // If scaleC=0, p=0 (False) -> Instruction SKIP (NOP)
-        // If scaleC=1, p=1 (True)  -> Instruction EXECUTE (Accumulate)
-        // We must ALWAYS execute the instruction.
-        uint32_t scaleC = 1u;
+        // scaleC controls accumulate mode via predicate p:
+        //   scaleC=0 -> p=false -> D = A*B       (ZERO mode, first iteration)
+        //   scaleC=1 -> p=true  -> D = A*B + C   (ACCUMULATE mode, subsequent)
+        // CRITICAL: First K-tile MUST use scaleC=0 to zero uninitialized TMEM!
+        uint32_t scaleC = (k_tile == 0) ? 0u : 1u;
         
         if (elect_one_sync_local()) {
             asm volatile(
@@ -1469,7 +1424,7 @@ void launch_fp4_gemm_optimized(
     torch::Tensor A, torch::Tensor B,
     torch::Tensor SFA, torch::Tensor SFB,
     torch::Tensor D,
-    int64_t M, int64_t N, int64_t K, int64_t L, int64_t K_scales_padded
+    int64_t M, int64_t N, int64_t K, int64_t L, int64_t K_scales
 ) {
     const uint8_t* A_ptr = A.data_ptr<uint8_t>();
     const uint8_t* B_ptr = B.data_ptr<uint8_t>();
@@ -1532,20 +1487,19 @@ void launch_fp4_gemm_optimized(
         }
     }
 
-    // SFA: to_blocked output layout - tiles of (32, 16) = 512 bytes each
-    // to_blocked: reshape(-1, 32, 16).flatten() produces 512-byte tiles
-    // Total tiles = n_row_blocks * n_col_blocks * 4 (from to_blocked permute structure)
+    // SFA: atom-tiled layout (32, 4, rest_m, 4, rest_k) flattened
+    // This matches sfa_permuted from reference.py and kutte.py
     {
-        int n_row_blocks = (M + 127) / 128;  // ceil_div(M, 128)
-        int n_col_blocks = (K_scales_padded + 3) / 4;  // ceil_div(K_scales, 4)
-        // to_blocked produces: n_row_blocks * n_col_blocks * 4 tiles of size (32, 16)
-        int total_tiles = n_row_blocks * n_col_blocks * 4;
+        int rest_m = (M + 127) / 128;  // ceil_div(M, 128)
+        int rest_k = (K_scales + 3) / 4;  // ceil_div(K_scales, 4)
+        // Total bytes = 32 * 4 * rest_m * 4 * rest_k
+        cuuint64_t total_bytes = static_cast<cuuint64_t>(32) * 4 * rest_m * 4 * rest_k;
         
-        // 2D layout: [16 bytes per row, 32 * total_tiles rows]
-        cuuint64_t total_rows = static_cast<cuuint64_t>(32) * total_tiles;
+        // 2D descriptor: [16 bytes width, total_bytes/16 rows]
+        cuuint64_t total_rows = total_bytes / 16;
         cuuint64_t dims_SFA[2] = {16, total_rows};
-        cuuint32_t box_SFA[2] = {16, 32};  // 16 bytes × 32 rows = 512 bytes
-        cuuint64_t strides_SFA[1] = {16};  // stride = 16 bytes between rows
+        cuuint32_t box_SFA[2] = {16, 32};  // Load 512 bytes per TMA (32 rows × 16 bytes)
+        cuuint64_t strides_SFA[1] = {16};
         
         CUresult resSFA = encode_tma_matrix(map_SFA_ptr, CU_TENSOR_MAP_DATA_TYPE_UINT8,
                                      2, const_cast<void*>(static_cast<const void*>(SFA_ptr)),
@@ -1559,15 +1513,15 @@ void launch_fp4_gemm_optimized(
         }
     }
 
-    // SFB: to_blocked output layout - tiles of (32, 16) = 512 bytes each
+    // SFB: atom-tiled layout (32, 4, rest_n, 4, rest_k) flattened
     {
-        int n_row_blocks = (N + 127) / 128;  // ceil_div(N, 128)
-        int n_col_blocks = (K_scales_padded + 3) / 4;  // ceil_div(K_scales, 4)
-        int total_tiles = n_row_blocks * n_col_blocks * 4;
+        int rest_n = (N + 127) / 128;  // ceil_div(N, 128)
+        int rest_k = (K_scales + 3) / 4;  // ceil_div(K_scales, 4)
+        cuuint64_t total_bytes = static_cast<cuuint64_t>(32) * 4 * rest_n * 4 * rest_k;
         
-        cuuint64_t total_rows = static_cast<cuuint64_t>(32) * total_tiles;
+        cuuint64_t total_rows = total_bytes / 16;
         cuuint64_t dims_SFB[2] = {16, total_rows};
-        cuuint32_t box_SFB[2] = {16, 32};  // 16 bytes × 32 rows = 512 bytes
+        cuuint32_t box_SFB[2] = {16, 32};
         cuuint64_t strides_SFB[1] = {16};
         
         CUresult resSFB = encode_tma_matrix(map_SFB_ptr, CU_TENSOR_MAP_DATA_TYPE_UINT8,
@@ -1634,7 +1588,7 @@ void launch_fp4_gemm_optimized(
     int N_int = static_cast<int>(N);
     int K_int = static_cast<int>(K);
     int L_int = static_cast<int>(L);
-    int K_scales_padded_int = static_cast<int>(K_scales_padded);
+    int K_scales_int = static_cast<int>(K_scales);
 
     void* kernel_args[] = {
         const_cast<uint8_t**>(&A_ptr),
@@ -1650,7 +1604,7 @@ void launch_fp4_gemm_optimized(
         &N_int,
         &K_int,
         &L_int,
-        &K_scales_padded_int
+        &K_scales_int
     };
 
     check_cuda(cudaLaunchKernel(kernel_ptr, grid, block, kernel_args, shared_bytes, 0), "cudaLaunchKernel");
@@ -1917,33 +1871,25 @@ def custom_kernel(data: input_t) -> output_t:
     a_bytes = a_2d.view(torch.uint8)
     b_bytes = b_2d.view(torch.uint8)
 
-    # =========================================================================
-    # SCALE FACTORS: Use to_blocked() output for tcgen05.mma byte ordering
-    # =========================================================================
-    # Debug showed sfa_permuted.flatten() has different byte order than to_blocked()
-    # (only 26% bytes match). tcgen05.mma expects to_blocked() byte order.
-    #
-    # to_blocked transforms: view(n_row_blocks, 128, n_col_blocks, 4).permute(0,2,1,3)
-    #   -> reshape(-1, 4, 32, 4).transpose(1, 2).reshape(-1, 32, 16).flatten()
+    # SCALE FACTORS: Use atom-tiled layout (sfa_permuted/sfb_permuted)
+    # =========================================================================  
+    # Match kutte.py which uses sfa_permuted: [32, 4, rest_m, 4, rest_k, l]
+    # This is BlockScaledBasicChunk atom layout from blockscaled_layout.py
+    # atom_shape = ((32, 4), (sf_vec_size=16, 4))
     
-    from reference import to_blocked
+    # Extract L=0 slice and flatten to 1D for TMA
+    # Shape: (32, 4, rest_m, 4, rest_k) -> flattened 1D
+    sfa_bytes = sfa_permuted[..., 0].flatten().contiguous().view(torch.uint8)
+    sfb_bytes = sfb_permuted[..., 0].flatten().contiguous().view(torch.uint8)
     
-    # Apply to_blocked to get correct byte ordering for tcgen05
-    sfa_blocked = to_blocked(sfa_ref_cpu[:, :, 0])  # Returns flattened 1D tensor
-    sfb_blocked = to_blocked(sfb_ref_cpu[:, :, 0])
-    
-    # Convert to uint8 bytes
-    sfa_bytes = sfa_blocked.view(torch.uint8).contiguous()
-    sfb_bytes = sfb_blocked.view(torch.uint8).contiguous()
-    
-    # Calculate K_scales_padded from actual dimensions
+    # IMPORTANT: Pass actual K_scales (not padded) for TMA descriptor
+    # atom-tiled layout uses actual K_scales, so TMA must match
     K_scales = K // 16
-    K_scales_padded = max(128, ((K_scales + 127) // 128) * 128)
 
     mod = get_module()
     mod.launch_fp4_gemm_optimized(
         a_bytes, b_bytes, sfa_bytes, sfb_bytes, c[:, :, 0],
-        M, N, K, 1, K_scales_padded  # L=1 always
+        M, N, K, 1, K_scales  # Pass actual K_scales for TMA dimensions
     )
 
     return c