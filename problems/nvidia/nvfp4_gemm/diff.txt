git --no-pager diff submission.py
diff --git a/problems/nvidia/nvfp4_gemm/submission.py b/problems/nvidia/nvfp4_gemm/submission.py
index 39ccfcd..f1fc24d 100644
--- a/problems/nvidia/nvfp4_gemm/submission.py
+++ b/problems/nvidia/nvfp4_gemm/submission.py
@@ -427,6 +427,58 @@ __device__ __forceinline__ void copy_sf_smem_to_tmem(
     );
 }
 
+// =========================================================================
+// TILE-LOCAL SCALE PERMUTATION FOR TCGEN05
+// =========================================================================
+// tcgen05.mma.block_scale expects scales in a specific "blocked" layout
+// that matches Python's to_blocked() function. This function applies the
+// same permutation to per-tile scale data after TMA loads simple scales.
+//
+// Python to_blocked for [128, 16] tile:
+//   view(1, 128, 4, 4).permute(0,2,1,3).reshape(-1,4,32,4).transpose(1,2).reshape(-1,32,16).flatten()
+//
+// Resulting index mapping:
+//   dst_idx = kb * 512 + (m % 32) * 16 + (m / 32) * 4 + k_in_block
+// where:
+//   kb = k / 4  (K-block index, 0..3)
+//   k_in_block = k % 4  (within K-block, 0..3)
+//   m = row index (0..127)
+//
+template<int TileM, int KScalesTile>
+__device__ __forceinline__ void permute_scales_to_blocked(
+    const uint8_t* __restrict__ src,  // Simple layout [TileM, SrcStride] with SrcStride >= KScalesTile
+    uint8_t* __restrict__ dst,        // Blocked layout [TileM * KScalesTile] contiguous
+    int SrcStride,                    // Stride between rows in src (SfaBoxK = 128)
+    int k_scale_offset,               // K-scale offset within SMEM (e.g., 0, 16, 32, ... for each K-tile)
+    int tid,                          // Thread ID
+    int num_threads                   // Total threads
+) {
+    // Total elements = TileM * KScalesTile = 128 * 16 = 2048
+    constexpr int TOTAL = TileM * KScalesTile;
+    
+    // Each thread processes multiple elements
+    #pragma unroll 4
+    for (int dst_idx = tid; dst_idx < TOTAL; dst_idx += num_threads) {
+        // Decode blocked index to source coordinates
+        // dst_idx = kb * 512 + m_in_group * 16 + m_group * 4 + k_in_block
+        int kb = dst_idx / 512;              // 0..3 (for KScalesTile=16)
+        int remaining = dst_idx % 512;
+        int m_in_group = remaining / 16;     // 0..31
+        int combined = remaining % 16;
+        int m_group = combined / 4;          // 0..3
+        int k_in_block = combined % 4;
+        
+        // Reconstruct original (m, k) coordinates
+        int m = m_in_group + m_group * 32;   // 0..127
+        // k is relative to current K-tile, add offset to get actual column in SMEM
+        int k = k_scale_offset + (kb * 4 + k_in_block);  // e.g., [0,15], [16,31], [32,47], ...
+        
+        // Read from simple layout, write to blocked layout
+        int src_idx = m * SrcStride + k;
+        dst[dst_idx] = src[src_idx];
+    }
+}
+
 // Elect one thread in warp to execute (CuTe-style elect_one_sync)
 __device__ __forceinline__ bool elect_one_sync_local() {
     uint32_t pred = 0;
@@ -1063,6 +1115,7 @@ fp4_gemm_rank2_cta(
     // - No manual FP4->FP16 decode needed!
 
     __shared__ uint32_t tmem_base_ptr_tcgen05;
+    __shared__ uint64_t acc_mbar_storage[2];  // Accumulator mbarrier for tcgen05.mma sync
 
     // Prologue: prefetch stages 0..StageCount-2 (same as classic path)
     int phase = 0;
@@ -1101,6 +1154,13 @@ fp4_gemm_rank2_cta(
     __syncthreads();
     uint32_t tmem_c = tmem_base_ptr_tcgen05;
     
+    // Initialize accumulator mbarrier
+    if (tid == 0) {
+        mbarrier_init(acc_mbar_storage);
+    }
+    __syncthreads();
+    uint32_t acc_mbar_addr = cvta_to_shared_u32(acc_mbar_storage);
+    
     // TMEM addresses for scale factors
     // TMEM addressing format: bits [15:0] = column index, bits [31:16] = DP (data path)
     // For block_scale.block16, each K-tile needs TileM × (TileK/16) = 128 × 16 = 2048 scale bytes
@@ -1121,6 +1181,22 @@ fp4_gemm_rank2_cta(
     uint32_t tmem_sfa_base = tmem_c + ACCUM_COLS;           // SFA at column 128
     uint32_t tmem_sfb_base = tmem_c + ACCUM_COLS + SF_COLS; // SFB at column 144
 
+    // =========================================================================
+    // PERMUTED SCALE BUFFERS FOR TCGEN05
+    // =========================================================================
+    // TMA loads simple scales into sfa_stage/sfb_stage (128-byte row stride).
+    // tcgen05.mma expects "blocked" layout scales in TMEM.
+    // We allocate separate contiguous buffers for permuted scales, then
+    // call permute_scales_to_blocked to convert and tcgen05.cp to copy to TMEM.
+    //
+    // Size: TileM * (TileK/16) = 128 * 16 = 2048 bytes each
+    constexpr int KScalesTile = TileK / 16;  // 16 for TileK=256
+    constexpr int PERMUTED_SF_SIZE = TileM * KScalesTile;  // 2048 bytes
+    
+    // Use static shared memory for permuted buffers (single-buffered, reused each K-tile)
+    __shared__ uint8_t sfa_permuted_smem[PERMUTED_SF_SIZE];
+    __shared__ uint8_t sfb_permuted_smem[PERMUTED_SF_SIZE];
+
     #if __CUDA_ARCH__ >= 1000
     // Build instruction descriptor (constant for all K-tiles)
     // sf_format = 0 for E4M3 scale format
@@ -1155,18 +1231,51 @@ fp4_gemm_rank2_cta(
         __syncthreads();
 
         #if __CUDA_ARCH__ >= 1000
-        // Copy scale factors from SMEM to TMEM
-        // Using tcgen05.cp.cta_group::1.128x128b which copies 128 rows × 16 bytes = 2KB at once
-        // Only one elected thread issues the copy (ThrID=1 in CuTe copy traits)
+        // =========================================================================
+        // STEP 1: Permute scales from simple layout to blocked layout
+        // =========================================================================
+        // TMA loaded simple scales into sfa_stage[stage] with 128-byte row stride.
+        // We need to permute to contiguous blocked layout for tcgen05.mma.
+        //
+        // CRITICAL: Each K-tile (TileK=256) needs different scale bytes.
+        // k_scale_offset tells which 16 bytes to read from each row in SMEM.
+        // For k_tile=0: offset=0 (read bytes 0-15)
+        // For k_tile=256: offset=16 (read bytes 16-31)
+        // For k_tile=512: offset=32 (read bytes 32-47)
+        // etc.
+        int k_scale_offset = (k_tile / 16) % SfaBoxK;  // 0, 16, 32, ..., wraps at SfaBoxK=128
+        
+        permute_scales_to_blocked<TileM, KScalesTile>(
+            sfa_stage[stage],       // Source: simple layout [TileM, K_scales] with SfaBoxK stride
+            sfa_permuted_smem,      // Dest: blocked layout, contiguous
+            SfaBoxK,                // Source stride = 128
+            k_scale_offset,         // K-scale offset for current K-tile
+            tid,
+            Threads
+        );
+        permute_scales_to_blocked<TileM, KScalesTile>(
+            sfb_stage[stage],       // Source: simple layout [TileN, K_scales] with SfaBoxK stride  
+            sfb_permuted_smem,      // Dest: blocked layout, contiguous
+            SfaBoxK,                // Source stride = 128
+            k_scale_offset,         // K-scale offset for current K-tile
+            tid,
+            Threads
+        );
+        __syncthreads();  // Ensure permutation complete before tcgen05.cp
+        
+        // =========================================================================
+        // STEP 2: Copy permuted scales from SMEM to TMEM
+        // =========================================================================
+        // The permuted buffers are contiguous (no row stride), so leading_byte_offset = 16
+        // (16 bytes per logical row in the blocked format)
         if (elect_one_sync_local()) {
-            // Build SMEM descriptors for scale factors
-            // Scale factors in SMEM: [TileM/N, SfaBoxK] layout
-            // Leading byte offset = SfaBoxK (128 bytes between rows)
-            uint64_t desc_sfa = make_sf_smem_desc(sfa_stage[stage], SfaBoxK, 0);
-            uint64_t desc_sfb = make_sf_smem_desc(sfb_stage[stage], SfaBoxK, 0);
+            // Build SMEM descriptors for PERMUTED scale factors
+            // Permuted layout: contiguous 2048 bytes, interpreted as 128 rows × 16 bytes
+            // Leading byte offset = 16 (bytes between rows in permuted format)
+            uint64_t desc_sfa = make_sf_smem_desc(sfa_permuted_smem, 16, 0);
+            uint64_t desc_sfb = make_sf_smem_desc(sfb_permuted_smem, 16, 0);
             
             // tcgen05.cp.cta_group::1.128x128b copies 128 data paths × 128 bits = 2KB
-            // This copies all scale factors for 128 rows at once
             asm volatile(
                 "tcgen05.cp.cta_group::1.128x128b [%0], %1;\n"
                 :: "r"(tmem_sfa_base), "l"(desc_sfa)
@@ -1199,8 +1308,8 @@ fp4_gemm_rank2_cta(
         );
 
         // Issue tcgen05.mma.kind::mxf4nvf4.block_scale.block16
-        // scaleC = 1 for first K-tile (clear accumulator), 0 otherwise
-        uint32_t scaleC = (k_tile == 0) ? 1u : 0u;
+        // scaleC = 0 for first K-tile (initialize/clear accumulator), 1 for subsequent tiles (accumulate)
+        uint32_t scaleC = (k_tile == 0) ? 0u : 1u;
         
         if (elect_one_sync_local()) {
             asm volatile(
@@ -1225,78 +1334,81 @@ fp4_gemm_rank2_cta(
         }
     }
 
+    // ========================================================================
+    // COMMIT: Signal that all tcgen05.mma operations are issued
+    // ========================================================================
+    #if __CUDA_ARCH__ >= 1000
+    if (elect_one_sync_local()) {
+        // tcgen05.commit signals that all previous tcgen05.mma operations
+        // for this CTA-group should be tracked by the mbarrier
+        asm volatile(
+            "tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [%0];\n"
+            :: "r"(acc_mbar_addr)
+            : "memory"
+        );
+    }
+    __syncthreads();
+    #endif
+
+    // ========================================================================
+    // WAIT: Ensure all async tcgen05.mma operations complete before reading TMEM
+    // ========================================================================
+    #if __CUDA_ARCH__ >= 1000
+    // Wait for the accumulator mbarrier (phase 0 since we only arrive once)
+    mbarrier_wait_parity(acc_mbar_storage, 0);
+    __syncthreads();
+    #endif
+
     // ========================================================================
     // EPILOGUE: TMEM -> register -> global D
     // ========================================================================
     {
-        // TMEM layout: 128 rows × 256 cols (we use first 128 cols for accumulator)
+        // TMEM layout: 128 rows × 128 cols for accumulator
         // SM100_TMEM_LOAD_16dp256b1x loads 16 data paths (rows) at once.
-        // All 16 threads in a subpartition call it together; each gets its own row's data.
-        
-        int subpart_in_warp = lane_id / 16;  // 0 or 1
-        int lane_in_subpart = lane_id % 16;  // 0..15
-        int global_subpart = warp_id * 2 + subpart_in_warp;  // 0..7 across CTA (for 4 warps)
+        // Each call gives ALL 16 lanes data at the SAME column range, but different rows.
+        // Lane i gets 4 floats for row (row_base + i) at columns [col, col+3].
+        //
+        // To cover 128 columns, we need 128/4 = 32 separate loads per row set.
+        // With 8 subpartitions handling 16 rows each, each subpart does 32 loads.
         
-        // Each of 8 subpartitions handles 16 rows (128 rows / 8 subparts)
-        // The 16dp instruction loads all 16 rows at once - each thread gets its own row
-        int row_base = global_subpart * 16;
-        int my_row_in_tile = row_base + lane_in_subpart;  // This thread's row within tile (0..127)
-        int global_row = m_tile + my_row_in_tile;
-        
-        // TMEM address: base column for this subpartition's 16 rows
-        // The DP (row) is implicitly handled by the 16dp instruction - all threads
-        // in subpart call with SAME address, hardware routes rows 0-15 to lanes 0-15
-        // So we need row_base in the DP field, not my_row_in_tile
-        uint32_t tmem_row_base = tmem_c + (row_base << 16);
-        
-        // Load first 64 columns (lanes 0-15 each get 4 floats for their own row)
-        {
-            uint32_t d0, d1, d2, d3;
-            cute::SM100::TMEM::LOAD::SM100_TMEM_LOAD_16dp256b1x::copy(
-                tmem_row_base, d0, d1, d2, d3);
-            
-            // Each thread gets 4 consecutive floats at cols [lane*4, lane*4+3]
-            int col_base = lane_in_subpart * 4;
-            float f0 = __uint_as_float(d0);
-            float f1 = __uint_as_float(d1);
-            float f2 = __uint_as_float(d2);
-            float f3 = __uint_as_float(d3);
+        if (warp_id < 4) {
+            int subpart_in_warp = lane_id / 16;  // 0 or 1
+            int lane_in_subpart = lane_id % 16;  // 0..15
+            int global_subpart = warp_id * 2 + subpart_in_warp;  // 0..7 for first 4 warps
             
-            if (global_row < M) {
-                int gc0 = n_tile + col_base;
-                int gc1 = n_tile + col_base + 1;
-                int gc2 = n_tile + col_base + 2;
-                int gc3 = n_tile + col_base + 3;
-                
-                if (gc0 < N) D[global_row * N + gc0] = __float2half(f0);
-                if (gc1 < N) D[global_row * N + gc1] = __float2half(f1);
-                if (gc2 < N) D[global_row * N + gc2] = __float2half(f2);
-                if (gc3 < N) D[global_row * N + gc3] = __float2half(f3);
-            }
-        }
-        
-        // Load second 64 columns (offset by 64 in column field)
-        {
-            uint32_t d0, d1, d2, d3;
-            cute::SM100::TMEM::LOAD::SM100_TMEM_LOAD_16dp256b1x::copy(
-                tmem_row_base + 64, d0, d1, d2, d3);
+            // Each of 8 subpartitions handles 16 rows (128 rows / 8 subparts)
+            int row_base = global_subpart * 16;
+            int my_row_in_tile = row_base + lane_in_subpart;
+            int global_row = m_tile + my_row_in_tile;
             
-            int col_base = 64 + lane_in_subpart * 4;
-            float f0 = __uint_as_float(d0);
-            float f1 = __uint_as_float(d1);
-            float f2 = __uint_as_float(d2);
-            float f3 = __uint_as_float(d3);
+            // TMEM address: row_base in DP field (bits 31:16), column in low bits
+            uint32_t tmem_row_base = tmem_c + (row_base << 16);
             
-            if (global_row < M) {
-                int gc0 = n_tile + col_base;
-                int gc1 = n_tile + col_base + 1;
-                int gc2 = n_tile + col_base + 2;
-                int gc3 = n_tile + col_base + 3;
+            // Loop over all 32 column groups (128 columns / 4 cols per load)
+            #pragma unroll
+            for (int col_group = 0; col_group < 32; ++col_group) {
+                int col_base = col_group * 4;  // 0, 4, 8, ..., 124
+                
+                uint32_t d0, d1, d2, d3;
+                cute::SM100::TMEM::LOAD::SM100_TMEM_LOAD_16dp256b1x::copy(
+                    tmem_row_base + col_base, d0, d1, d2, d3);
+                
+                float f0 = __uint_as_float(d0);
+                float f1 = __uint_as_float(d1);
+                float f2 = __uint_as_float(d2);
+                float f3 = __uint_as_float(d3);
                 
-                if (gc0 < N && col_base < TileN) D[global_row * N + gc0] = __float2half(f0);
-                if (gc1 < N && col_base + 1 < TileN) D[global_row * N + gc1] = __float2half(f1);
-                if (gc2 < N && col_base + 2 < TileN) D[global_row * N + gc2] = __float2half(f2);
-                if (gc3 < N && col_base + 3 < TileN) D[global_row * N + gc3] = __float2half(f3);
+                if (global_row < M) {
+                    int gc0 = n_tile + col_base;
+                    int gc1 = n_tile + col_base + 1;
+                    int gc2 = n_tile + col_base + 2;
+                    int gc3 = n_tile + col_base + 3;
+                    
+                    if (gc0 < N) D[global_row * N + gc0] = __float2half(f0);
+                    if (gc1 < N) D[global_row * N + gc1] = __float2half(f1);
+                    if (gc2 < N) D[global_row * N + gc2] = __float2half(f2);
+                    if (gc3 < N) D[global_row * N + gc3] = __float2half(f3);
+                }
             }
         }
     }
@@ -1761,16 +1873,22 @@ def custom_kernel(data: input_t) -> output_t:
     a_bytes = a_2d.view(torch.uint8)
     b_bytes = b_2d.view(torch.uint8)
 
-    # Extract 2D slices for scales
-    # Simple scales: [M/N, K/16, L] -> remove L dimension
-    sfa_2d = sfa_ref_cpu[..., 0].contiguous()
-    sfb_2d = sfb_ref_cpu[..., 0].contiguous()
+    # =========================================================================
+    # SCALE FACTORS: Use simple layout (kernel does permutation for tcgen05)
+    # =========================================================================
+    # The kernel has two paths:
+    # - Non-tcgen05 (mma.sync): Uses simple scales directly
+    # - tcgen05: Permutes simple scales to blocked layout in SMEM via permute_scales_to_blocked
+    #
+    # We pass simple scales [M, K_scales] / [N, K_scales] to TMA.
     
-    # K_scales_padded for SWIZZLE_128B (must be at least 128 bytes)
+    sfa_2d = sfa_ref_cpu[..., 0].contiguous()  # [M, K_scales]
+    sfb_2d = sfb_ref_cpu[..., 0].contiguous()  # [N, K_scales]
+    
+    # K_scales_padded for SWIZZLE (must be at least 128 bytes)
     K_scales_padded = max(128, ((K_scales + 127) // 128) * 128)
     
-    # CRITICAL: Pad scale tensors to match K_scales_padded
-    # TMA will try to load K_scales_padded bytes per row, so tensors must have that width
+    # Pad K dimension to K_scales_padded
     if sfa_2d.shape[1] < K_scales_padded:
         padding = K_scales_padded - sfa_2d.shape[1]
         sfa_2d = torch.nn.functional.pad(sfa_2d, (0, padding), value=0)
@@ -1778,9 +1896,9 @@ def custom_kernel(data: input_t) -> output_t:
         padding = K_scales_padded - sfb_2d.shape[1]
         sfb_2d = torch.nn.functional.pad(sfb_2d, (0, padding), value=0)
     
-    # Launch kernel with 2D tensors
-    sfa_bytes = sfa_2d.view(torch.uint8)
-    sfb_bytes = sfb_2d.view(torch.uint8)
+    # Launch kernel with simple scale layout (kernel permutes for tcgen05)
+    sfa_bytes = sfa_2d.view(torch.uint8).contiguous()
+    sfb_bytes = sfb_2d.view(torch.uint8).contiguous()
 
     mod = get_module()
     mod.launch_fp4_gemm_optimized(