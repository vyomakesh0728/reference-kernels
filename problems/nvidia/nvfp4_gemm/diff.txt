git --no-pager diff submission.py
diff --git a/problems/nvidia/nvfp4_gemm/submission.py b/problems/nvidia/nvfp4_gemm/submission.py
index a94ac9e..f4dc834 100644
--- a/problems/nvidia/nvfp4_gemm/submission.py
+++ b/problems/nvidia/nvfp4_gemm/submission.py
@@ -31,10 +31,12 @@ cuda_source = r"""
 #include "cutlass/numeric_types.h"
 #include "cutlass/arch/barrier.h"
 #include "cutlass/detail/sm100_tmem_helper.hpp"
+#include "cutlass/detail/sm100_blockscaled_layout.hpp"
 #include <cute/arch/copy_sm100.hpp>
 #include <cute/tensor.hpp>
 #include <cute/atom/mma_atom.hpp>
 #include <cute/arch/mma_sm100_umma.hpp>
+#include <cute/arch/mma_sm100_desc.hpp>
 
 using cutlass::half_t;
 
@@ -483,45 +485,25 @@ __device__ __forceinline__ uint64_t make_instr_desc_mxf4(
     uint32_t tmem_sfa,       // Used to derive SF ID bits
     uint32_t tmem_sfb        // Used to derive SF ID bits
 ) {
-    uint32_t desc = 0;
-    
-    // Format value for E2M1 (NVFP4) is 5
-    constexpr uint32_t E2M1_FORMAT = 5;
-    
-    // [0:2) sparse_id2 = 0
-    // [2:3) sparse_flag = 0
-    // [3:4) reserved
-    // [4:6) b_sf_id = 0 (always 0 for single CTA)
-    // [6:7) reserved
-    // For this kernel we allocate SF in the default SF region and do not use multi-region SF.
-    // Keep SF IDs at 0 to avoid depending on allocator-specific TMEM address high bits.
-    uint32_t a_sf_id = 0;
-    uint32_t b_sf_id = 0;
-    // [4:6) b_sf_id
-    desc |= ((b_sf_id & 0x3u) << 4);
-
-    // [7:10) a_format = 5 (E2M1)
-    desc |= (E2M1_FORMAT << 7);
-    // [10:13) b_format = 5 (E2M1)
-    desc |= (E2M1_FORMAT << 10);
-    // [13:14) a_negate = 0
-    // [14:15) b_negate = 0
-    // [15:16) a_major
-    desc |= ((a_major & 0x1) << 15);
-    // [16:17) b_major
-    desc |= ((b_major & 0x1) << 16);
-    // [17:23) n_dim = tile_n >> 3
-    desc |= (((tile_n >> 3) & 0x3F) << 17);
-    // [23:24) scale_format
-    desc |= ((sf_format & 0x1) << 23);
-    // [24:29) m_dim = tile_m >> 4
-    desc |= (((tile_m >> 4) & 0x1F) << 24);
-    // [29:31) a_sf_id
-    desc |= ((a_sf_id & 0x3u) << 29);
-    // [31:32) k_size = 0 (K64 for MXF4 dense)
-    
-    // idescE: upper 32 bits are the instruction descriptor
-    return ((uint64_t)desc << 32);
+    // Canonical descriptor construction + runtime SF-ID derivation.
+    (void)sf_format;
+    using AType = cutlass::float_e2m1_t;
+    using BType = cutlass::float_e2m1_t;
+    using CType = float;
+    using SFType = cutlass::float_ue4m3_t;
+
+    auto desc_i = cute::UMMA::make_instr_desc_block_scaled<
+        AType, BType, CType, SFType,
+        128, 128,
+        cute::UMMA::Major::K, cute::UMMA::Major::K
+    >();
+    desc_i.m_dim_ = uint32_t(tile_m >> 4);
+    desc_i.n_dim_ = uint32_t(tile_n >> 3);
+    desc_i.a_major_ = uint8_t(a_major & 0x1);
+    desc_i.b_major_ = uint8_t(b_major & 0x1);
+
+    // Populate a_sf_id/b_sf_id from TMEM addresses (as CUTLASS does).
+    return cute::UMMA::make_runtime_instr_desc_block_scaled(desc_i, tmem_sfa, tmem_sfb);
 }
 
 // Prefetch tile using TMA - simplified for Rank-2 (L=1) GEMM
@@ -999,6 +981,23 @@ fp4_gemm_rank2_cta(
         offset += SF_TILE_BYTES;
     }
 
+#if USE_tcgen05_MAINLOOP
+    // Compact scale tiles for tcgen05.cp: ensure byte-for-byte packed16 order.
+    align_up_smem_128();
+    uint8_t* sfa_compact_stage[StageCount];
+    for (int s = 0; s < StageCount; ++s) {
+        sfa_compact_stage[s] = smem + offset;
+        offset += SF_TILE_BYTES;
+    }
+
+    align_up_smem_128();
+    uint8_t* sfb_compact_stage[StageCount];
+    for (int s = 0; s < StageCount; ++s) {
+        sfb_compact_stage[s] = smem + offset;
+        offset += SF_TILE_BYTES;
+    }
+#endif
+
     #if !USE_tcgen05_MAINLOOP
     // =========================================================================
     // FP16 decode buffers - ONLY needed for non-tcgen05 path
@@ -1217,23 +1216,19 @@ fp4_gemm_rank2_cta(
     static_assert(TileK % kKBlock == 0, "TileK must be divisible by kKBlock");
     constexpr int kNumKBlocks = TileK / kKBlock;
 
-    // tmem_sf_frg expects ((MMA_MN, (VecSize, NSF)), num_MMA_MN, num_MMA_K)
-    constexpr int kSfVecSize = 16;
-    constexpr int kSfNSF = kKBlock / kSfVecSize;  // 4 for K=64, vec=16
-    static_assert(kSfVecSize * kSfNSF == kKBlock, "Scale-factor NSF mismatch");
-    auto tmem_shape_sf = make_shape(
-        make_shape(Int<TileM>{}, make_shape(Int<kSfVecSize>{}, Int<kSfNSF>{})),
-        Int<1>{},
-        Int<kNumKBlocks>{}
-    );
+    // Canonical TMEM scale tensors: match CUTLASS' blockscaled collective.
+    // Allocate the TMEM scale fragments using the derived SMEM scale layout atoms.
+    using BlkScaledCfg = cutlass::detail::Sm1xxBlockScaledConfig<16>;
+    using TileShape_MNK = cute::Shape<cute::Int<TileM>, cute::Int<TileN>, cute::Int<TileK>>;
+    using SmemLayoutAtomSFA = decltype(BlkScaledCfg::deduce_smem_layoutSFA(tiled_mma, TileShape_MNK{}));
+    using SmemLayoutAtomSFB = decltype(BlkScaledCfg::deduce_smem_layoutSFB(tiled_mma, TileShape_MNK{}));
 
-    auto tCtSFA = make_tensor<typename MMA_Traits<MmaOp>::FrgTypeSFA>(tmem_shape_sf);
-    auto tCtSFB = make_tensor<typename MMA_Traits<MmaOp>::FrgTypeSFB>(tmem_shape_sf);
+    auto tCtSFA = make_tensor<typename MMA_Traits<MmaOp>::FrgTypeSFA>(shape(SmemLayoutAtomSFA{}));
+    auto tCtSFB = make_tensor<typename MMA_Traits<MmaOp>::FrgTypeSFB>(shape(SmemLayoutAtomSFB{}));
 
-    uint32_t tmem_sfa_base = tmem_c + cutlass::detail::find_tmem_tensor_col_offset(tCtAcc);
-    tCtSFA.data() = make_tmem_ptr<cutlass::float_ue4m3_t>(tmem_sfa_base);
-    uint32_t tmem_sfb_base = tmem_sfa_base + cutlass::detail::find_tmem_tensor_col_offset(tCtSFA);
-    tCtSFB.data() = make_tmem_ptr<cutlass::float_ue4m3_t>(tmem_sfb_base);
+    // Place SFA/SFB after the accumulator region in TMEM.
+    tCtSFA.data() = tCtAcc.data().get() + cutlass::detail::find_tmem_tensor_col_offset(tCtAcc);
+    tCtSFB.data() = tCtSFA.data().get() + cutlass::detail::find_tmem_tensor_col_offset(tCtSFA);
 
     __shared__ __align__(16) uint64_t desc_a_smem_sh[kNumKBlocks];
     __shared__ __align__(16) uint64_t desc_b_smem_sh[kNumKBlocks];
@@ -1257,11 +1252,14 @@ fp4_gemm_rank2_cta(
     // Build instruction descriptor (constant for all K-tiles)
     // sf_format = 0 for E4M3 scale format
     // a_sf_id and b_sf_id are set to 0 (default ID, not derived from addresses)
+    // Instruction descriptor (derive SF IDs from TMEM addresses)
+    uint32_t tmem_sfa0 = raw_pointer_cast(tCtSFA.data());
+    uint32_t tmem_sfb0 = raw_pointer_cast(tCtSFB.data());
     uint64_t idescE = make_instr_desc_mxf4(
         TileM, TileN,
         0, 0,       // K-major for both A and B
         0,          // sf_format = E4M3
-        tmem_sfa_base, tmem_sfb_base
+        tmem_sfa0, tmem_sfb0
     );
     uint32_t idescE_hi = uint32_t(idescE >> 32);
     #endif
@@ -1312,6 +1310,30 @@ fp4_gemm_rank2_cta(
         __syncthreads();
 
         #if __CUDA_ARCH__ >= 1000
+        // =========================================================================
+        // Compact scale tiles into packed16 order (byte-for-byte match to to_blocked)
+        // =========================================================================
+        {
+            constexpr int kScalePanelBytes = 32 * 16;
+            constexpr int kTotalScaleBytes = kNumKBlocks * kScalePanelBytes;
+            uint8_t* sfa_src = sfa_stage[stage];
+            uint8_t* sfb_src = sfb_stage[stage];
+            uint8_t* sfa_dst = sfa_compact_stage[stage];
+            uint8_t* sfb_dst = sfb_compact_stage[stage];
+            for (int idx = tid; idx < kTotalScaleBytes; idx += Threads) {
+                int mm32 = (idx & 511) >> 4;
+                int packed16 = (idx & 511) & 15;
+                int kb = idx >> 9;
+                int mm4 = packed16 >> 2;
+                int kk4 = packed16 & 3;
+                int row_global = mm32 + (mm4 << 5);
+                int scale_col = (kb << 2) + kk4;
+                sfa_dst[idx] = load_sf_tile_byte_2048<TileK>(sfa_src, row_global, scale_col);
+                sfb_dst[idx] = load_sf_tile_byte_2048<TileK>(sfb_src, row_global, scale_col);
+            }
+        }
+        __syncthreads();
+
         // =========================================================================
         // Scale SMEM -> TMEM (Cp4x32x128b): one copy per K-block (K=64)
         // =========================================================================
@@ -1321,11 +1343,12 @@ fp4_gemm_rank2_cta(
         // NOTE: Match kutte.py oracle issue scope: issue tcgen05.cp from a single warp
         // (warp 0) and a single lane (lane 0). This avoids redundant copies from
         // multiple warps in the CTA that can corrupt TMEM scale panels.
-        uint32_t sfa_stage_smem = cvta_to_shared_u32(sfa_stage[stage]);
-        uint32_t sfb_stage_smem = cvta_to_shared_u32(sfb_stage[stage]);
-        if (warp_id == 0 && lane_id == 0) {
-            // tcgen05.cp ... warpx4 is a broadcast op; do NOT manually replicate
-            // via DP shifts here. Issue exactly one UTCCP per 512B scale panel.
+        uint32_t sfa_stage_smem = cvta_to_shared_u32(sfa_compact_stage[stage]);
+        uint32_t sfb_stage_smem = cvta_to_shared_u32(sfb_compact_stage[stage]);
+        // Scale SMEM->TMEM copy is warpgroup-scoped (.warpx4). All threads in the
+        // 4-warp warpgroup must participate (no lane_id gating).
+        if (warp_id < 4) {
+            // Issue one UTCCP per 512B scale panel (per K-block).
             #pragma unroll
             for (int kb = 0; kb < kNumKBlocks; ++kb) {
                 uint64_t desc_sfa = make_umma_smem_desc_addr(
@@ -1350,16 +1373,17 @@ fp4_gemm_rank2_cta(
                     : "memory"
                 );
             }
-            // ADD tcgen05.commit here
-            tcgen05_commit_mbarrier(mbar_stage(mbar_cp, stage));
+            if (warp_id == 0 && lane_id == 0) {
+                tcgen05_commit_mbarrier(mbar_stage(mbar_cp, stage));
+            }
         }
 
-        // Ensure scale-panel TMEM stores are visible before tcgen05.mma reads them.
-        // Use all warps in the warpgroup to avoid relying on lane-level execution for fences.
-        if (warp_id < 4) {
-            asm volatile("tcgen05.wait::st.sync.aligned;\n" ::: "memory");
+        // Wait for scale copy completion before MMA consumes TMEM scales.
+        // Use the same parity as the current stage.
+        __syncthreads();
+        if (warp_id == 0 && lane_id == 0) {
+            mbarrier_wait_parity(mbar_stage(mbar_cp, stage), phase);
         }
-
         __syncthreads();
 
         // =========================================================================
@@ -1401,47 +1425,47 @@ fp4_gemm_rank2_cta(
             uint32_t tmem_sfb_kb = tmem_sfb_kb_sh[kb];
             uint32_t idesc_hi = idescE_hi_sh;
 
-            if (warp_id < 4) {
-                if (accum) {
-                    asm volatile(
-                        "{\n\t"
-                        ".reg .pred p;\n\t"
-                        "setp.ne.b32 p, 1, 0;\n\t"
-                        "tcgen05.mma.cta_group::1.kind::mxf4nvf4.block_scale.block16 "
-                        "[%0], %1, %2, %3, [%4], [%5], p;\n\t"
-                        "}\n"
-                        :
-                        : "r"(tmem_c), "l"(desc_a_smem), "l"(desc_b_smem),
-                          "r"(idesc_hi), "r"(tmem_sfa_kb), "r"(tmem_sfb_kb)
-                        : "memory"
-                    );
-                } else {
-                    asm volatile(
-                        "{\n\t"
-                        ".reg .pred p;\n\t"
-                        "setp.ne.b32 p, 0, 0;\n\t"
-                        "tcgen05.mma.cta_group::1.kind::mxf4nvf4.block_scale.block16 "
-                        "[%0], %1, %2, %3, [%4], [%5], p;\n\t"
-                        "}\n"
-                        :
-                        : "r"(tmem_c), "l"(desc_a_smem), "l"(desc_b_smem),
-                          "r"(idesc_hi), "r"(tmem_sfa_kb), "r"(tmem_sfb_kb)
-                        : "memory"
-                    );
-                }
+            if (accum) {
+                asm volatile(
+                    "{\n\t"
+                    ".reg .pred p;\n\t"
+                    "setp.ne.b32 p, 1, 0;\n\t"
+                    "tcgen05.mma.cta_group::1.kind::mxf4nvf4.block_scale.block16 "
+                    "[%0], %1, %2, %3, [%4], [%5], p;\n\t"
+                    "}\n"
+                    :
+                    : "r"(tmem_c), "l"(desc_a_smem), "l"(desc_b_smem),
+                      "r"(idesc_hi), "r"(tmem_sfa_kb), "r"(tmem_sfb_kb)
+                    : "memory"
+                );
+            } else {
+                asm volatile(
+                    "{\n\t"
+                    ".reg .pred p;\n\t"
+                    "setp.ne.b32 p, 0, 0;\n\t"
+                    "tcgen05.mma.cta_group::1.kind::mxf4nvf4.block_scale.block16 "
+                    "[%0], %1, %2, %3, [%4], [%5], p;\n\t"
+                    "}\n"
+                    :
+                    : "r"(tmem_c), "l"(desc_a_smem), "l"(desc_b_smem),
+                      "r"(idesc_hi), "r"(tmem_sfa_kb), "r"(tmem_sfb_kb)
+                    : "memory"
+                );
             }
         };
 
-        if (k_tile == 0) {
-            mma_kb(0, false);
-            #pragma unroll
-            for (int kb = 1; kb < kNumKBlocks; ++kb) {
-                mma_kb(kb, true);
-            }
-        } else {
-            #pragma unroll
-            for (int kb = 0; kb < kNumKBlocks; ++kb) {
-                mma_kb(kb, true);
+        if (warp_id < 4) {
+            if (k_tile == 0) {
+                mma_kb(0, false);
+                #pragma unroll
+                for (int kb = 1; kb < kNumKBlocks; ++kb) {
+                    mma_kb(kb, true);
+                }
+            } else {
+                #pragma unroll
+                for (int kb = 0; kb < kNumKBlocks; ++kb) {
+                    mma_kb(kb, true);
+                }
             }
         }
 
@@ -1496,14 +1520,16 @@ fp4_gemm_rank2_cta(
                     for (uint32_t i = 0; i < STORE_BLOCK_N / kNumElemsPerBankGroup; ++i) {
                         // TMEM address: N-dimension offset from base
                         // SM100_TMEM_LOAD is warp-collective: same address gives different rows to different threads
-                        uint32_t tmem_addr = tmem_c +
-                                             accum_stage_idx * kNumMWaves * BLOCK_N +
+                        // TMEM load address: column offset.
+                        // For SM100 TMEM load ops (as used in sm100_bf16_gemm.cuh), the address is an offset
+                        // in TMEM columns; do not add the tcgen05.alloc base pointer.
+                        uint32_t tmem_addr = accum_stage_idx * kNumMWaves * BLOCK_N +
                                              w * BLOCK_N +
                                              s * STORE_BLOCK_N + i * kNumElemsPerBankGroup;
 
                         // Load 4 consecutive FP32 values from TMEM
                         uint32_t v0, v1, v2, v3;
-                        cute::SM100_TMEM_LOAD_32dp32b4x::copy(tmem_addr, v0, v1, v2, v3);
+                        cute::SM100_TMEM_LOAD_32dp32b4x::copy(tmem_c + tmem_addr, v0, v1, v2, v3);
                         cutlass::arch::fence_view_async_tmem_load();
 
                         // Global memory N coordinate (row-major):