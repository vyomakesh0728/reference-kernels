cuda-gdb --args python3 test_correctness.py --only 1 --debug-umma
NVIDIA (R) cuda-gdb 13.0
Portions Copyright (C) 2007-2025 NVIDIA Corporation
Based on GNU gdb 14.2
Copyright (C) 2023 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
Type "show copying" and "show warranty" for details.
This CUDA-GDB was configured as "x86_64-pc-linux-gnu".
Type "show configuration" for configuration details.
For bug reporting instructions, please see:
<https://forums.developer.nvidia.com/c/developer-tools/cuda-developer-tools/cuda-gdb>.
Find the CUDA-GDB manual and other documentation resources online at:
    <https://docs.nvidia.com/cuda/cuda-gdb/index.html>.

For help, type "help".
Type "apropos word" to search for commands related to "word"...
Reading symbols from python3...
(No debugging symbols found in python3)
(cuda-gdb) run
Starting program: /usr/bin/python3 test_correctness.py --only 1 --debug-umma
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/usr/lib/x86_64-linux-gnu/libthread_db.so.1".
[New Thread 0x7fff439ff640 (LWP 14029)]
[New Thread 0x7fff431fe640 (LWP 14030)]
[New Thread 0x7fff409fd640 (LWP 14031)]
[New Thread 0x7fff3e1fc640 (LWP 14032)]
[New Thread 0x7fff3b9fb640 (LWP 14033)]
[New Thread 0x7fff391fa640 (LWP 14034)]
[New Thread 0x7fff369f9640 (LWP 14035)]
[New Thread 0x7fff361f8640 (LWP 14036)]
[New Thread 0x7fff319f7640 (LWP 14037)]
[New Thread 0x7fff2f1f6640 (LWP 14038)]
[New Thread 0x7fff2c9f5640 (LWP 14039)]
[New Thread 0x7fff2a1f4640 (LWP 14040)]
[New Thread 0x7fff279f3640 (LWP 14041)]
[New Thread 0x7fff251f2640 (LWP 14042)]
[New Thread 0x7fff229f1640 (LWP 14043)]
[New Thread 0x7fff201f0640 (LWP 14044)]
[New Thread 0x7fff1d9ef640 (LWP 14045)]
[New Thread 0x7fff1b1ee640 (LWP 14046)]
[New Thread 0x7fff189ed640 (LWP 14047)]
[New Thread 0x7fff161ec640 (LWP 14048)]
[New Thread 0x7fff139eb640 (LWP 14049)]
[New Thread 0x7fff111ea640 (LWP 14050)]
[New Thread 0x7fff0e9e9640 (LWP 14051)]
[New Thread 0x7fff0c1e8640 (LWP 14052)]
[New Thread 0x7fff099e7640 (LWP 14053)]
[New Thread 0x7fff071e6640 (LWP 14054)]
[New Thread 0x7fff049e5640 (LWP 14055)]
[New Thread 0x7fff021e4640 (LWP 14056)]
[New Thread 0x7ffeff9e3640 (LWP 14057)]
[New Thread 0x7ffef85dc640 (LWP 14058)]
[New Thread 0x7ffef730b640 (LWP 14059)]
[Thread 0x7fff1b1ee640 (LWP 14046) exited]
[Thread 0x7ffeff9e3640 (LWP 14057) exited]
[Thread 0x7fff021e4640 (LWP 14056) exited]
[Thread 0x7fff049e5640 (LWP 14055) exited]
[Thread 0x7fff071e6640 (LWP 14054) exited]
[Thread 0x7fff099e7640 (LWP 14053) exited]
[Thread 0x7fff0c1e8640 (LWP 14052) exited]
[Thread 0x7fff0e9e9640 (LWP 14051) exited]
[Thread 0x7fff111ea640 (LWP 14050) exited]
[Thread 0x7fff139eb640 (LWP 14049) exited]
[Thread 0x7fff161ec640 (LWP 14048) exited]
[Thread 0x7fff189ed640 (LWP 14047) exited]
[Thread 0x7fff1d9ef640 (LWP 14045) exited]
[Thread 0x7fff201f0640 (LWP 14044) exited]
[Thread 0x7fff229f1640 (LWP 14043) exited]
[Thread 0x7fff251f2640 (LWP 14042) exited]
[Thread 0x7fff279f3640 (LWP 14041) exited]
[Thread 0x7fff2a1f4640 (LWP 14040) exited]
[Thread 0x7fff2c9f5640 (LWP 14039) exited]
[Thread 0x7fff2f1f6640 (LWP 14038) exited]
[Thread 0x7fff319f7640 (LWP 14037) exited]
[Thread 0x7fff361f8640 (LWP 14036) exited]
[Thread 0x7fff369f9640 (LWP 14035) exited]
[Thread 0x7fff391fa640 (LWP 14034) exited]
[Thread 0x7fff3b9fb640 (LWP 14033) exited]
[Thread 0x7fff3e1fc640 (LWP 14032) exited]
[Thread 0x7fff409fd640 (LWP 14031) exited]
[Thread 0x7fff431fe640 (LWP 14030) exited]
[Thread 0x7fff439ff640 (LWP 14029) exited]
[Detaching after fork from child process 14060]
================================================================================
NVFP4 GEMM Correctness Validation
================================================================================

Running 1 test cases...
Tolerance: rtol=1e-03, atol=1e-03


[Test 1/1] M=128, N=256, K=256, L=1
------------------------------------------------------------
  Generating input data...
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
whimsical-wealthy-chipmunk: CUDA coredump is not supported with other CUDA debugging tools. Disabling coredump.
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
[New Thread 0x7ffeff9e3640 (LWP 14068)]
  to_blocked(sfa_ref_cpu)[0..31]: 44 44 44 44 44 40 40 44 44 38 44 00 00 38 40 00 44 40 40 38 38 38 44 00 40 38 44 00 38 00 38 40
  to_blocked(sfb_ref_cpu)[0..31]: 38 44 38 38 38 40 40 38 40 00 00 00 38 38 00 00 40 38 00 00 00 38 44 38 00 44 38 00 40 38 38 40
  a_ref packed bytes[0..31]: b3 78 6c e7 89 52 29 d3 c0 5f fb f5 a8 d3 ce 36 70 a3 ca 84 bb 87 fa bf 8d da 72 28 c2 00 19 51
  b_ref packed bytes[0..31]: 76 7d a9 a8 a6 25 10 1b 88 00 ef a6 7d 6c 20 73 8b c6 74 e4 ba b9 ac ce 09 ef 5f 7e 8e a4 68 c1
  Running custom kernel...
[Detaching after vfork from child process 14069]
[Detaching after vfork from child process 14070]
[Detaching after vfork from child process 14071]
SFA size=[32,4,1,4,4] stride=[16,4,2048,1,512] K_scales=16
SFA kScaleChunksPerTile=4
SFB size=[32,4,2,4,4] stride=[16,4,2048,1,512] K_scales=16
SFB kScaleChunksPerTile=4

Thread 1 "python3" received signal SIGTRAP, Trace/breakpoint trap.
[Switching focus to CUDA kernel 0, grid 52, block (0,0,0), thread (0,0,0), device 0, sm 142, warp 1, lane 0]
0x00007ffcb9805d60 in fp4_gemm_rank2_cta<128, 256, 128><<<(1,2,1),(128,1,1)>>> () at /root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu:897
897	        asm volatile("brkpt;");
(cuda-gdb) info cuda threads
  BlockIdx ThreadIdx To BlockIdx ThreadIdx Count                 PC                                                                   Filename  Line 
Kernel 0
*  (0,0,0)   (0,0,0)     (0,0,0)  (31,0,0)    32 0x00007ffcb9805d60 /root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu   897 
   (0,0,0)  (32,0,0)     (0,0,0)  (63,0,0)    32 0x00007ffcb9806380 /root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu   975 
   (0,0,0)  (64,0,0)     (0,0,0)  (95,0,0)    32 0x00007ffcb98063c0 /root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu   982 
   (0,0,0)  (96,0,0)     (0,0,0) (127,0,0)    32 0x00007ffcb9806390 /root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu   975 
(cuda-gdb) print blockIdx 
$1 = {x = 0, y = 0, z = 0}
(cuda-gdb) print threadIdx
$2 = {x = 0, y = 0, z = 0}
(cuda-gdb) print m_tile print n_tile print k_tile print stage
No symbol "m_tile" in current context.
(cuda-gdb) print m_tile
No symbol "m_tile" in current context.
(cuda-gdb) print n_tile
No symbol "n_tile" in current context.
(cuda-gdb) print k_tile
No symbol "k_tile" in current context.
(cuda-gdb) print stage
No symbol "stage" in current context.
(cuda-gdb) print tmem_c
No symbol "tmem_c" in current context.
(cuda-gdb) print desc_a_smem_sh[0
No symbol "desc_a_smem_sh" in current context.
(cuda-gdb) print desc_b_smem_sh[0]
No symbol "desc_b_smem_sh" in current context.
(cuda-gdb) continue
Continuing.
idescE_kb0=0x0820168000000000 fmt(a,b)=5,5 major(a,b)=0,0 dims(m,n)=8,16 sf_fmt=0 sf_id(a,b)=0,0 tmem_sfa=0x00000080 tmem_sfb=0x00000090
idescE_kb1=0x0820168000000000 fmt(a,b)=5,5 major(a,b)=0,0 dims(m,n)=8,16 sf_fmt=0 sf_id(a,b)=0,0 tmem_sfa=0x00000084 tmem_sfb=0x00000094
idescE_kb2=0x0820168000000000 fmt(a,b)=5,5 major(a,b)=0,0 dims(m,n)=8,16 sf_fmt=0 sf_id(a,b)=0,0 tmem_sfa=0x00000088 tmem_sfb=0x00000098
idescE_kb3=0x0820168000000000 fmt(a,b)=5,5 major(a,b)=0,0 dims(m,n)=8,16 sf_fmt=0 sf_id(a,b)=0,0 tmem_sfa=0x0000008c tmem_sfb=0x0000009c
mma_scale_vec=16 opcode=block16
sfa_stage[0..31]: 44 44 44 44 44 40 40 44 44 38 44 00 00 38 40 00 44 40 40 38 38 38 44 00 40 38 44 00 38 00 38 40
sfa_compact[0..31]: 44 44 44 44 44 40 40 44 44 38 44 00 00 38 40 00 44 40 40 38 38 38 44 00 40 38 44 00 38 00 38 40
sfb_compact[0..31]: 38 44 38 38 38 40 40 38 40 00 00 00 38 38 00 00 40 38 00 00 00 38 44 38 00 44 38 00 40 38 38 40
a_smem_base=0x00000800 a_k1=0x00000801 a_m1=0x00000880
b_smem_base=0x00008800 b_k1=0x00008801 b_n1=0x00008880
a_stride_k=1 a_stride_m=128
b_stride_k=1 b_stride_n=128
ElementAB bits=4 TileKPacked=128
a_packed_stage[0..3]: b3 78 6c e7
b_packed_stage[0..3]: 76 7d a9 a8
sA_full k[0..7], m0: 03 0b 08 07 0c 06 07 0e
sB_full k[0..7], n0: 06 07 0d 07 09 0a 08 0a
desc_a_smem[0]=0x4000400000080080
desc_b_smem[0]=0x4000400000080880
desc_a_smem start=0x0080 lbo=0x0008 sbo=0x0000 layout=2
desc_b_smem start=0x0880 lbo=0x0008 sbo=0x0000 layout=2
desc_expected_a lbo=0x0008 sbo=0x0000
desc_expected_b lbo=0x0008 sbo=0x0000
tmem_sfa0_addr=0x00000080 tmem_sfb0_addr=0x00000090
tmem_c_base=0x00000000
map gm=0 gn=0 tmem_addr=0 v=8697.750000 -6123.500000 -283.250000 17688.500000
D[0..7]: 8696.000 -6124.000 -283.250 17696.000 -13000.000 -10336.000 -7964.000 -24640.000
  ref D[0,0..3]: 932.000 -1.000 -531.000 266.500
  Validating against reference...
  ✗ FAILED: mismatch found! custom implementation doesn't match reference: Number of mismatched elements: 32767 ERROR AT (0, 0, 0): 8696.0 932.0 ERROR AT (0, 1, 0): -6124.0 -1.0 ERROR AT (0, 2, 0): -283.25 -531.0 ERROR AT (0, 3, 0): 17696.0 266.5 ERROR AT (0, 4, 0): -13000.0 -712.0 ... and 32762 more mismatched elements.

================================================================================
SUMMARY
================================================================================
Total tests: 1
Passed:      0 (0%)
Failed:      1 (100%)

✗ 1 test(s) failed
[Thread 0x7ffef730b640 (LWP 14059) exited]
[Thread 0x7ffeff9e3640 (LWP 14068) exited]
[Thread 0x7ffff7c5d480 (LWP 14025) exited]
[Thread 0x7ffef85dc640 (LWP 14058) exited]
[New process 14025]
[Inferior 1 (process 14025) exited with code 01]
(cuda-gdb) print tmem_addr
No symbol "tmem_addr" in current context.
(cuda-gdb) print gm
No symbol "gm" in current context.
(cuda-gdb) print gn
No symbol "gn" in current context.
(cuda-gdb) run
Starting program: /usr/bin/python3 test_correctness.py --only 1 --debug-umma
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/usr/lib/x86_64-linux-gnu/libthread_db.so.1".
[New Thread 0x7fff439ff640 (LWP 14493)]
[New Thread 0x7fff431fe640 (LWP 14494)]
[New Thread 0x7fff409fd640 (LWP 14495)]
[New Thread 0x7fff3c1fc640 (LWP 14496)]
[New Thread 0x7fff3b9fb640 (LWP 14497)]
[New Thread 0x7fff391fa640 (LWP 14498)]
[New Thread 0x7fff369f9640 (LWP 14499)]
[New Thread 0x7fff341f8640 (LWP 14500)]
[New Thread 0x7fff339f7640 (LWP 14501)]
[New Thread 0x7fff2f1f6640 (LWP 14502)]
[New Thread 0x7fff2c9f5640 (LWP 14503)]
[New Thread 0x7fff2a1f4640 (LWP 14504)]
[New Thread 0x7fff279f3640 (LWP 14505)]
[New Thread 0x7fff251f2640 (LWP 14506)]
[New Thread 0x7fff229f1640 (LWP 14507)]
[New Thread 0x7fff201f0640 (LWP 14508)]
[New Thread 0x7fff1d9ef640 (LWP 14509)]
[New Thread 0x7fff1b1ee640 (LWP 14510)]
[New Thread 0x7fff189ed640 (LWP 14511)]
[New Thread 0x7fff161ec640 (LWP 14512)]
[New Thread 0x7fff139eb640 (LWP 14513)]
[New Thread 0x7fff111ea640 (LWP 14514)]
[New Thread 0x7fff0e9e9640 (LWP 14515)]
[New Thread 0x7fff0c1e8640 (LWP 14516)]
[New Thread 0x7fff099e7640 (LWP 14517)]
[New Thread 0x7fff071e6640 (LWP 14518)]
[New Thread 0x7fff049e5640 (LWP 14519)]
[New Thread 0x7fff021e4640 (LWP 14520)]
[New Thread 0x7ffeff9e3640 (LWP 14521)]
[New Thread 0x7ffef85dc640 (LWP 14522)]
[New Thread 0x7ffef730b640 (LWP 14523)]
[Thread 0x7fff201f0640 (LWP 14508) exited]
[Thread 0x7fff409fd640 (LWP 14495) exited]
[Thread 0x7fff339f7640 (LWP 14501) exited]
[Thread 0x7fff071e6640 (LWP 14518) exited]
[Thread 0x7ffeff9e3640 (LWP 14521) exited]
[Thread 0x7fff049e5640 (LWP 14519) exited]
[Thread 0x7fff021e4640 (LWP 14520) exited]
[Thread 0x7fff099e7640 (LWP 14517) exited]
[Thread 0x7fff0c1e8640 (LWP 14516) exited]
[Thread 0x7fff0e9e9640 (LWP 14515) exited]
[Thread 0x7fff111ea640 (LWP 14514) exited]
[Thread 0x7fff139eb640 (LWP 14513) exited]
[Thread 0x7fff161ec640 (LWP 14512) exited]
[Thread 0x7fff189ed640 (LWP 14511) exited]
[Thread 0x7fff1b1ee640 (LWP 14510) exited]
[Thread 0x7fff1d9ef640 (LWP 14509) exited]
[Thread 0x7fff229f1640 (LWP 14507) exited]
[Thread 0x7fff251f2640 (LWP 14506) exited]
[Thread 0x7fff279f3640 (LWP 14505) exited]
[Thread 0x7fff2a1f4640 (LWP 14504) exited]
[Thread 0x7fff2c9f5640 (LWP 14503) exited]
[Thread 0x7fff2f1f6640 (LWP 14502) exited]
[Thread 0x7fff341f8640 (LWP 14500) exited]
[Thread 0x7fff369f9640 (LWP 14499) exited]
[Thread 0x7fff391fa640 (LWP 14498) exited]
[Thread 0x7fff3b9fb640 (LWP 14497) exited]
[Thread 0x7fff3c1fc640 (LWP 14496) exited]
[Thread 0x7fff431fe640 (LWP 14494) exited]
[Thread 0x7fff439ff640 (LWP 14493) exited]
[Detaching after fork from child process 14524]
================================================================================
NVFP4 GEMM Correctness Validation
================================================================================

Running 1 test cases...
Tolerance: rtol=1e-03, atol=1e-03


[Test 1/1] M=128, N=256, K=256, L=1
------------------------------------------------------------
  Generating input data...
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Corrupted shared library list: 0x55555c4b5340 != 0x55555c486d90
whimsical-wealthy-chipmunk: CUDA coredump is not supported with other CUDA debugging tools. Disabling coredump.
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
warning: Cuda Driver error detected: No CUDA context is current to the calling thread
warning: Cuda Driver error detected: Returning 201 (CUDA_ERROR_INVALID_CONTEXT) from cuCtxGetDevice_v2
[New Thread 0x7ffeff9e3640 (LWP 14532)]
  to_blocked(sfa_ref_cpu)[0..31]: 44 44 44 44 44 40 40 44 44 38 44 00 00 38 40 00 44 40 40 38 38 38 44 00 40 38 44 00 38 00 38 40
  to_blocked(sfb_ref_cpu)[0..31]: 38 44 38 38 38 40 40 38 40 00 00 00 38 38 00 00 40 38 00 00 00 38 44 38 00 44 38 00 40 38 38 40
  a_ref packed bytes[0..31]: b3 78 6c e7 89 52 29 d3 c0 5f fb f5 a8 d3 ce 36 70 a3 ca 84 bb 87 fa bf 8d da 72 28 c2 00 19 51
  b_ref packed bytes[0..31]: 76 7d a9 a8 a6 25 10 1b 88 00 ef a6 7d 6c 20 73 8b c6 74 e4 ba b9 ac ce 09 ef 5f 7e 8e a4 68 c1
  Running custom kernel...
[Detaching after vfork from child process 14533]
[Detaching after vfork from child process 14534]
[Detaching after vfork from child process 14535]
  ✗ ERROR: Error building extension 'nvfp4_gemm_sm100_ptx_dbg': [1/2] /usr/local/cuda-13.0/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=nvfp4_gemm_sm100_ptx_dbg -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-13.0/include -isystem /usr/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DNDEBUG --use_fast_math --ftz=true --prec-div=false --prec-sqrt=false --fmad=true -std=c++17 -gencode=arch=compute_100a,code=sm_100a --expt-relaxed-constexpr --expt-extended-lambda -Xcudafe --diag_suppress=20012 -maxrregcount=128 --ptxas-options=-v,-warn-lmem-usage -lineinfo -I/usr/local/cutlass/include -DNVFP4_DEBUG_DUMP=1 -c /root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu -o cuda.cuda.o 
FAILED: [code=1] cuda.cuda.o 
/usr/local/cuda-13.0/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=nvfp4_gemm_sm100_ptx_dbg -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-13.0/include -isystem /usr/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DNDEBUG --use_fast_math --ftz=true --prec-div=false --prec-sqrt=false --fmad=true -std=c++17 -gencode=arch=compute_100a,code=sm_100a --expt-relaxed-constexpr --expt-extended-lambda -Xcudafe --diag_suppress=20012 -maxrregcount=128 --ptxas-options=-v,-warn-lmem-usage -lineinfo -I/usr/local/cutlass/include -DNVFP4_DEBUG_DUMP=1 -c /root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu -o cuda.cuda.o 
/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(898): error: identifier "K_packed" is undefined
                 m_tile, n_tile, batch, K, K_scales, K_packed);
                                                     ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(1608): warning #177-D: variable "LAYOUT_AD_M" was declared but never referenced
          constexpr uint32_t LAYOUT_AD_M = 128;
                             ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(1616): warning #177-D: variable "kHasShortcut" was declared but never referenced
          constexpr bool kHasShortcut = (kSwizzleCDMode / kNumBankGroupBytes) == 8;
                         ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(877): warning #177-D: variable "SfaBoxK" was declared but never referenced
      constexpr int SfaBoxK = 128;
                    ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(879): warning #177-D: variable "a_stride" was declared but never referenced
      constexpr int a_stride = TileK + 8;
                    ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(880): warning #177-D: variable "b_stride" was declared but never referenced
      constexpr int b_stride = TileK + 8;
                    ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(886): warning #177-D: variable "is_consumer" was declared but never referenced
      const bool is_consumer = true;
                 ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(904): warning #177-D: variable "tile_rows" was declared but never referenced
      const int tile_rows = (M - m_tile) < TileM ? (M - m_tile) : TileM;
                ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(905): warning #177-D: variable "tile_cols" was declared but never referenced
      const int tile_cols = (N - n_tile) < TileN ? (N - n_tile) : TileN;
                ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(517): warning #177-D: variable "c_k_scales" was declared but never referenced
              uint32_t c_k_scales = static_cast<uint32_t>(k_tile_base >> 4);
                       ^
          detected during instantiation of "void fp4_gemm_rank2_cta<TileM,TileK,Threads>(const uint8_t *, const uint8_t *, const uint8_t *, const uint8_t *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, half *, int, int, int, int, int) [with TileM=128, TileK=256, Threads=128]" 

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(509): warning #177-D: variable "SfaBoxK" was declared but never referenced
      constexpr int SfaBoxK = 128;
                    ^
          detected during instantiation of "void fp4_gemm_rank2_cta<TileM,TileK,Threads>(const uint8_t *, const uint8_t *, const uint8_t *, const uint8_t *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, half *, int, int, int, int, int) [with TileM=128, TileK=256, Threads=128]" 

1 error detected in the compilation of "/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu".
ninja: build stopped: subcommand failed.


================================================================================
SUMMARY
================================================================================
Total tests: 1
Passed:      0 (0%)
Failed:      1 (100%)

✗ 1 test(s) failed
[Thread 0x7ffef730b640 (LWP 14523) exited]
[Thread 0x7ffef85dc640 (LWP 14522) exited]
[Thread 0x7ffff7c5d480 (LWP 14491) exited]
[Thread 0x7ffeff9e3640 (LWP 14532) exited]
[New process 14491]
[Inferior 1 (process 14491) exited with code 01]
(cuda-gdb) quit