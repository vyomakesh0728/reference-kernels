python3 test_correctness.py --only 1 --debug-umma
================================================================================
NVFP4 GEMM Correctness Validation
================================================================================

Running 1 test cases...
Tolerance: rtol=1e-03, atol=1e-03


[Test 1/1] M=128, N=256, K=256, L=1
------------------------------------------------------------
  Generating input data...
  to_blocked(sfa_ref_cpu)[0..31]: 44 44 44 44 44 40 40 44 44 38 44 00 00 38 40 00 44 40 40 38 38 38 44 00 40 38 44 00 38 00 38 40
  to_blocked(sfb_ref_cpu)[0..31]: 38 44 38 38 38 40 40 38 40 00 00 00 38 38 00 00 40 38 00 00 00 38 44 38 00 44 38 00 40 38 38 40
  a_ref packed bytes[0..31]: b3 78 6c e7 89 52 29 d3 c0 5f fb f5 a8 d3 ce 36 70 a3 ca 84 bb 87 fa bf 8d da 72 28 c2 00 19 51
  b_ref packed bytes[0..31]: 76 7d a9 a8 a6 25 10 1b 88 00 ef a6 7d 6c 20 73 8b c6 74 e4 ba b9 ac ce 09 ef 5f 7e 8e a4 68 c1
  Running custom kernel...
  ✗ ERROR: Error building extension 'nvfp4_gemm_sm100_ptx_dbg': [1/2] /usr/local/cuda-13.0/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=nvfp4_gemm_sm100_ptx_dbg -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-13.0/include -isystem /usr/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DNDEBUG --use_fast_math --ftz=true --prec-div=false --prec-sqrt=false --fmad=true -std=c++17 -gencode=arch=compute_100a,code=sm_100a --expt-relaxed-constexpr --expt-extended-lambda -Xcudafe --diag_suppress=20012 -maxrregcount=128 --ptxas-options=-v,-warn-lmem-usage -lineinfo -I/usr/local/cutlass/include -DNVFP4_DEBUG_DUMP=1 -c /root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu -o cuda.cuda.o 
FAILED: [code=1] cuda.cuda.o 
/usr/local/cuda-13.0/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=nvfp4_gemm_sm100_ptx_dbg -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-13.0/include -isystem /usr/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DNDEBUG --use_fast_math --ftz=true --prec-div=false --prec-sqrt=false --fmad=true -std=c++17 -gencode=arch=compute_100a,code=sm_100a --expt-relaxed-constexpr --expt-extended-lambda -Xcudafe --diag_suppress=20012 -maxrregcount=128 --ptxas-options=-v,-warn-lmem-usage -lineinfo -I/usr/local/cutlass/include -DNVFP4_DEBUG_DUMP=1 -c /root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu -o cuda.cuda.o 
/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(1600): warning #177-D: variable "LAYOUT_AD_M" was declared but never referenced
          constexpr uint32_t LAYOUT_AD_M = 128;
                             ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(1608): warning #177-D: variable "kHasShortcut" was declared but never referenced
          constexpr bool kHasShortcut = (kSwizzleCDMode / kNumBankGroupBytes) == 8;
                         ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(877): warning #177-D: variable "SfaBoxK" was declared but never referenced
      constexpr int SfaBoxK = 128;
                    ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(879): warning #177-D: variable "a_stride" was declared but never referenced
      constexpr int a_stride = TileK + 8;
                    ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(880): warning #177-D: variable "b_stride" was declared but never referenced
      constexpr int b_stride = TileK + 8;
                    ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(886): warning #177-D: variable "is_consumer" was declared but never referenced
      const bool is_consumer = true;
                 ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(896): warning #177-D: variable "tile_rows" was declared but never referenced
      const int tile_rows = (M - m_tile) < TileM ? (M - m_tile) : TileM;
                ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(897): warning #177-D: variable "tile_cols" was declared but never referenced
      const int tile_cols = (N - n_tile) < TileN ? (N - n_tile) : TileN;
                ^

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(517): warning #177-D: variable "c_k_scales" was declared but never referenced
              uint32_t c_k_scales = static_cast<uint32_t>(k_tile_base >> 4);
                       ^
          detected during instantiation of "void fp4_gemm_rank2_cta<TileM,TileK,Threads>(const uint8_t *, const uint8_t *, const uint8_t *, const uint8_t *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, half *, int, int, int, int, int) [with TileM=128, TileK=256, Threads=128]" 

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(509): warning #177-D: variable "SfaBoxK" was declared but never referenced
      constexpr int SfaBoxK = 128;
                    ^
          detected during instantiation of "void fp4_gemm_rank2_cta<TileM,TileK,Threads>(const uint8_t *, const uint8_t *, const uint8_t *, const uint8_t *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, half *, int, int, int, int, int) [with TileM=128, TileK=256, Threads=128]" 

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(1452): error: function "cute::stride<Is...,Fn,O,Layout>(const cute::ComposedLayout<Fn, O, Layout> &) [with Is=<>, Fn=cute::Swizzle<3, 4, 3>, O=cute::smem_ptr_flag_bits<4>, Layout=cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::_256, cute::_1>>, cute::tuple<cute::tuple<cute::_256, cute::C<2048>>, cute::tuple<cute::_1, cute::C<0>>>>]" (declared at line 241 of /usr/local/cutlass/include/cute/layout_composed.hpp) cannot be referenced -- it is a deleted function
              auto a_stride = stride(smem_layout_a);
                              ^
          detected during instantiation of "void fp4_gemm_rank2_cta<TileM,TileK,Threads>(const uint8_t *, const uint8_t *, const uint8_t *, const uint8_t *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, half *, int, int, int, int, int) [with TileM=128, TileK=256, Threads=128]" 

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(1453): error: function "cute::stride<Is...,Fn,O,Layout>(const cute::ComposedLayout<Fn, O, Layout> &) [with Is=<>, Fn=cute::Swizzle<3, 4, 3>, O=cute::smem_ptr_flag_bits<4>, Layout=cute::Layout<cute::tuple<cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::_256, cute::_1>>, cute::tuple<cute::tuple<cute::_256, cute::C<2048>>, cute::tuple<cute::_1, cute::C<0>>>>]" (declared at line 241 of /usr/local/cutlass/include/cute/layout_composed.hpp) cannot be referenced -- it is a deleted function
              auto b_stride = stride(smem_layout_b);
                              ^
          detected during instantiation of "void fp4_gemm_rank2_cta<TileM,TileK,Threads>(const uint8_t *, const uint8_t *, const uint8_t *, const uint8_t *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, half *, int, int, int, int, int) [with TileM=128, TileK=256, Threads=128]" 

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(1459): error: no suitable conversion function from "std::conditional_t<true, cute::tuple_element_t<0UL, cute::tuple<cute::tuple<cute::Int<8>, cute::Int<16>>, cute::tuple<cute::_256, cute::_1>>>, cute::tuple_element_t<0UL, cute::tuple<cute::tuple<cute::Int<8>, cute::Int<16>>, cute::tuple<cute::_256, cute::_1>>> &>" (aka "std::conditional_t<true, cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<8>, cute::C<16>> &>") to "int" exists
                     int(get<0>(a_shape)), int(get<1>(a_shape)),
                         ^
          detected during instantiation of "void fp4_gemm_rank2_cta<TileM,TileK,Threads>(const uint8_t *, const uint8_t *, const uint8_t *, const uint8_t *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, half *, int, int, int, int, int) [with TileM=128, TileK=256, Threads=128]" 

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(1459): error: no suitable conversion function from "std::conditional_t<true, cute::tuple_element_t<1UL, cute::tuple<cute::Shape<cute::_8, cute::_256>, cute::Stride<cute::_256, cute::_1>>>, cute::tuple_element_t<1UL, cute::tuple<cute::Shape<cute::_8, cute::_256>, cute::Stride<cute::_256, cute::_1>>> &>" (aka "std::conditional_t<true, cute::tuple<cute::C<256>, cute::C<1>>, cute::tuple<cute::C<256>, cute::C<1>> &>") to "int" exists
                     int(get<0>(a_shape)), int(get<1>(a_shape)),
                                               ^
          detected during instantiation of "void fp4_gemm_rank2_cta<TileM,TileK,Threads>(const uint8_t *, const uint8_t *, const uint8_t *, const uint8_t *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, half *, int, int, int, int, int) [with TileM=128, TileK=256, Threads=128]" 

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(1462): error: no suitable conversion function from "std::conditional_t<true, cute::tuple_element_t<0UL, cute::tuple<cute::tuple<cute::Int<8>, cute::Int<16>>, cute::tuple<cute::_256, cute::_1>>>, cute::tuple_element_t<0UL, cute::tuple<cute::tuple<cute::Int<8>, cute::Int<16>>, cute::tuple<cute::_256, cute::_1>>> &>" (aka "std::conditional_t<true, cute::tuple<cute::C<8>, cute::C<16>>, cute::tuple<cute::C<8>, cute::C<16>> &>") to "int" exists
                     int(get<0>(b_shape)), int(get<1>(b_shape)),
                         ^
          detected during instantiation of "void fp4_gemm_rank2_cta<TileM,TileK,Threads>(const uint8_t *, const uint8_t *, const uint8_t *, const uint8_t *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, half *, int, int, int, int, int) [with TileM=128, TileK=256, Threads=128]" 

/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu(1462): error: no suitable conversion function from "std::conditional_t<true, cute::tuple_element_t<1UL, cute::tuple<cute::Shape<cute::_8, cute::_256>, cute::Stride<cute::_256, cute::_1>>>, cute::tuple_element_t<1UL, cute::tuple<cute::Shape<cute::_8, cute::_256>, cute::Stride<cute::_256, cute::_1>>> &>" (aka "std::conditional_t<true, cute::tuple<cute::C<256>, cute::C<1>>, cute::tuple<cute::C<256>, cute::C<1>> &>") to "int" exists
                     int(get<0>(b_shape)), int(get<1>(b_shape)),
                                               ^
          detected during instantiation of "void fp4_gemm_rank2_cta<TileM,TileK,Threads>(const uint8_t *, const uint8_t *, const uint8_t *, const uint8_t *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, const CUtensorMap *, half *, int, int, int, int, int) [with TileM=128, TileK=256, Threads=128]" 

6 errors detected in the compilation of "/root/.cache/torch_extensions/py310_cu130/nvfp4_gemm_sm100_ptx_dbg/cuda.cu".
ninja: build stopped: subcommand failed.


================================================================================
SUMMARY
================================================================================
Total tests: 1
Passed:      0 (0%)
Failed:      1 (100%)

✗ 1 test(s) failed